{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Directories/ Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import find_folders\n",
    "import mne\n",
    "from scipy.stats import mannwhitneyu\n",
    "import math\n",
    "import funs_logistic_regression\n",
    "from importlib import reload\n",
    "import pickle\n",
    "\n",
    "import dat_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedrive = find_folders.get_onedrive_path()\n",
    "project_path = find_folders.get_onedrive_path(\"entrainment\")\n",
    "print(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw_accel = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\with_med_FTG\\\\files\\\\raw_accel_files'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle TMSI Accelerometer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CODE\n",
    "import tmsi_poly5reader\n",
    "\n",
    "poly5_name = 'Sub-029_18mfu_dysk_segNoEnt_ramp-20230216T104449.DATA.Poly5'\n",
    "\n",
    "# LOAD FILE\n",
    "accel = tmsi_poly5reader.Poly5Reader(\n",
    "    os.path.join(path_raw_accel,\n",
    "                 poly5_name)\n",
    ")\n",
    "\n",
    "raw_acc = accel.read_data_MNE()\n",
    "dn_accel = raw_acc.resample(250, npad=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_accel.ch_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dn_accel = dn_accel.get_data(picks=[6,7,8])\n",
    "my_dn_accel.shape\n",
    "plt.plot(my_dn_accel[0,:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(\n",
    "    path_raw_accel,\n",
    "    'Sub029_M1_RampUp_250Hz_LHAND.csv'\n",
    "), my_dn_accel, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_blocks_df = pd.read_excel(os.path.join(\n",
    "    project_path,\n",
    "    'results',\n",
    "    'accelerometer',\n",
    "    'Accel_Blocks_NoEntrainment.xlsx'\n",
    "))\n",
    "\n",
    "accel_blocks_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\subcohort_noENTRAINMENT\\\\retap_results\\\\features'\n",
    "blocks_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\subcohort_noENTRAINMENT\\\\retap_results\\\\extracted_tapblocks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features = ['freq', 'mean_raise_velocity', 'coefVar_raise_velocity', 'trace_RMSn', 'coefVar_intraTapInt', 'slope_intraTapInt','trace_entropy','jerkiness_trace', 'mean_tapRMS',\n",
    "                'coefVar_tapRMS', 'mean_impactRMS', 'coefVar_impactRMS', 'slope_impactRMS', 'coefVar_tap_entropy', 'slope_tap_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs = accel_blocks_df['Percept_ID'].unique()\n",
    "\n",
    "for sub in all_subs:\n",
    "    print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [1,4]\n",
    "\n",
    "for sub in all_subs:\n",
    "\n",
    "    this_sub = sub\n",
    "\n",
    "    for cond in conditions:\n",
    "\n",
    "        this_cond = cond\n",
    "\n",
    "        these_blocks = accel_blocks_df.loc[(accel_blocks_df['Percept_ID'] == this_sub) & (accel_blocks_df['Cond'] == this_cond), 'Blocks']\n",
    "        block_strings = ['block' + str(num) for num in these_blocks]\n",
    "        print(block_strings)\n",
    "\n",
    "        # Filter the json files based on conditions\n",
    "        jsons_to_import = []\n",
    "        for filename in os.listdir(features_path):\n",
    "            if filename.endswith('.json') and this_sub in filename:\n",
    "                for block_str in block_strings:\n",
    "                    if filename.endswith(f'{block_str}.json'):\n",
    "                        jsons_to_import.append(filename)\n",
    "\n",
    "        print(jsons_to_import)\n",
    "\n",
    "        # Filter the csv files based on conditions\n",
    "        csvs_to_import = []\n",
    "        for filename in os.listdir(blocks_path):\n",
    "            if filename.endswith('.csv') and this_sub in filename:\n",
    "                for block_str in block_strings:\n",
    "                    if filename.endswith(f'{block_str}_250Hz.csv'):\n",
    "                        csvs_to_import.append(filename)\n",
    "\n",
    "        print(csvs_to_import)\n",
    "\n",
    "        # Read the selected JSON files\n",
    "\n",
    "        combined_dict = []\n",
    "        for filename in jsons_to_import:\n",
    "            file_path = os.path.join(features_path, filename)\n",
    "            with open(file_path) as f:\n",
    "                this_block_feat = json.load(f)\n",
    "                matching_keys = set(imp_features) & set(this_block_feat.keys())\n",
    "                combined_dict.append({key: this_block_feat[key] for key in matching_keys})\n",
    "\n",
    "        #SAVE IT\n",
    "        suptitle = str(this_sub) + ' - Condition ' + str(this_cond)\n",
    "\n",
    "        file_name = \"\".join(suptitle.split()) + '_features.json'\n",
    "\n",
    "        #PLOT IT\n",
    "        %matplotlib qt\n",
    "        fig, axs = plt.subplots(len(csvs_to_import),1, figsize = (18,10))\n",
    "\n",
    "        suptitle = str(this_sub) + ' - Condition ' + str(this_cond)\n",
    "        fig.suptitle(suptitle, fontsize=14, fontweight='bold')\n",
    "\n",
    "        rounded_list = [\n",
    "            {key: round(value, 2) for key, value in dictionary.items()}\n",
    "            for dictionary in combined_dict\n",
    "        ]\n",
    "\n",
    "        for num, file in enumerate(csvs_to_import):\n",
    "            this_block_csv = pd.read_csv(os.path.join(blocks_path,file))\n",
    "            axs[num].plot(this_block_csv)\n",
    "            axs[num].set_ylabel('Acceleration [g] - ' + str(block_strings[num]))\n",
    "            axs[num].set_title(rounded_list[num])\n",
    "\n",
    "            plt.savefig(os.path.join(\n",
    "            project_path,\n",
    "            'results',\n",
    "            'accelerometer',\n",
    "            'inspections',\n",
    "            'group_NO_Entrainment',\n",
    "            \"\".join(suptitle.split()),\n",
    "            ), dpi = 250\n",
    "            )\n",
    "                \n",
    "            # Combine the file name with the current directory to create the file path\n",
    "            file_path = os.path.join(\n",
    "                project_path,\n",
    "                'results',\n",
    "                'accelerometer',\n",
    "                'inspections',\n",
    "                'group_NO_Entrainment')\n",
    "\n",
    "            # Serializing json\n",
    "            json_object = json.dumps(combined_dict, indent=4)\n",
    "            \n",
    "            # Writing to sample.json\n",
    "            with open(os.path.join(file_path,file_name), \"w\") as outfile:\n",
    "                outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, axs = plt.subplots(len(csvs_to_import),1, figsize = (18,10))\n",
    "\n",
    "suptitle = str(this_sub) + ' - Condition ' + str(this_cond)\n",
    "fig.suptitle(suptitle, fontsize=14, fontweight='bold')\n",
    "\n",
    "rounded_list = [\n",
    "    {key: round(value, 2) for key, value in dictionary.items()}\n",
    "    for dictionary in combined_dict\n",
    "]\n",
    "\n",
    "for num, file in enumerate(csvs_to_import):\n",
    "    this_block_csv = pd.read_csv(os.path.join(blocks_path,file))\n",
    "    axs[num].plot(this_block_csv)\n",
    "    axs[num].set_ylabel('Acceleration [g] - ' + str(block_strings[num]))\n",
    "    axs[num].set_title(rounded_list[num])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'results',\n",
    "    'accelerometer',\n",
    "    'inspections',\n",
    "    \"\".join(suptitle.split()),\n",
    "), dpi = 250\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average all blocks for each condition within subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = os.path.join(\n",
    "    project_path,\n",
    "    'results', 'accelerometer', 'inspections', 'Condition3'\n",
    ")\n",
    "\n",
    "all_features_cond1 = pd.DataFrame()\n",
    "\n",
    "# Loop through the JSON files in the directory\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        percept_id = filename[:6]\n",
    "\n",
    "        # Load the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # Initialize a dictionary to store the accumulated sums and counts for each key\n",
    "        summed_values = {}\n",
    "        count_values = {}\n",
    "        \n",
    "        # Iterate through the dictionaries in the JSON data\n",
    "        for data_dict in json_data:\n",
    "            # Accumulate the sums and counts for each key\n",
    "            for key, value in data_dict.items():\n",
    "                if key not in summed_values:\n",
    "                    summed_values[key] = value\n",
    "                    count_values[key] = 1\n",
    "                else:\n",
    "                    summed_values[key] += value\n",
    "                    count_values[key] += 1\n",
    "        \n",
    "        # Calculate the averages for each key\n",
    "        #averaged_values = {key: summed_values[key] / count_values[key] for key in summed_values}\n",
    "        averaged_values = {key: summed_values[key] for key in summed_values}\n",
    "        averaged_values['Percept_ID'] = percept_id\n",
    "        \n",
    "        # Append the averaged values to the dataframe\n",
    "        all_features_cond1 = all_features_cond1.append(averaged_values, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting dataframe\n",
    "all_features_cond1 = all_features_cond1.reindex(columns=['Percept_ID'] + list(all_features_cond1.columns[:-1]))\n",
    "print(all_features_cond1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as JSON\n",
    "json_file_path = os.path.join(json_dir, 'all_features_cond3.json')\n",
    "all_features_cond1.to_json(json_file_path, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all blocks together without averaging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = os.path.join(project_path, 'results', 'accelerometer', 'MotorPerf_Comps')\n",
    "\n",
    "filenames = os.listdir(json_dir)\n",
    "json_filenames = [filename for filename in filenames if not filename.startswith('.DS_Store')]\n",
    "\n",
    "data_frames = []\n",
    "for filename in json_filenames:\n",
    "    file_path = os.path.join(json_dir, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "        df.insert(0, 'Percept_ID',filename[:6])   # Extract filename without extension\n",
    "        data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single big DataFrame\n",
    "big_dataframe = pd.concat(data_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = os.path.join(json_dir, 'all_featureswithENTR_cond4_ALLBLOCKS.json')\n",
    "big_dataframe.to_json(json_file_path, orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all averaged values in four conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''json_dir = os.path.join(\n",
    "    project_path,\n",
    "    'results', 'accelerometer', 'inspections\\\\'\n",
    ")'''\n",
    "\n",
    "json_dir = os.path.join(project_path, 'results', 'accelerometer', 'inspections')\n",
    "\n",
    "feat_cond_1 = pd.read_json(os.path.join(json_dir,'all_features_cond1.json'))\n",
    "feat_cond_2 = pd.read_json(os.path.join(json_dir,'all_features_cond2.json'))\n",
    "feat_cond_3 = pd.read_json(os.path.join(json_dir,'all_features_cond3.json'))\n",
    "feat_cond_4 = pd.read_json(os.path.join(json_dir,'all_features_cond4.json'))\n",
    "\n",
    "\n",
    "fts_of_interest = ['Percept_ID',\"coefVar_intraTapInt\", \"trace_RMSn\", \"coefVar_impactRMS\", \"mean_raise_velocity\"]\n",
    "feat_cond_1 = feat_cond_1[fts_of_interest]\n",
    "feat_cond_2 = feat_cond_2[fts_of_interest]\n",
    "feat_cond_3 = feat_cond_3[fts_of_interest]\n",
    "feat_cond_4 = feat_cond_4[fts_of_interest]\n",
    "\n",
    "#feat_cond_4 = pd.read_json(str(json_dir) + 'all_features_cond4.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column values as NumPy arrays\n",
    "column1_values = feat_cond_1['Percept_ID'].values\n",
    "column2_values = feat_cond_2['Percept_ID'].values\n",
    "column3_values = feat_cond_3['Percept_ID'].values\n",
    "column4_values = feat_cond_4['Percept_ID'].values\n",
    "\n",
    "# Find the similar values between the three columns\n",
    "similar_values = np.intersect1d(column1_values, np.intersect1d(column2_values, column3_values))\n",
    "print(similar_values)\n",
    "\n",
    "# Keep only the rows with similar values in each dataframe\n",
    "feat_cond_1_filtered = feat_cond_1[feat_cond_1['Percept_ID'].isin(similar_values)]\n",
    "feat_cond_2_filtered = feat_cond_2[feat_cond_2['Percept_ID'].isin(similar_values)]\n",
    "feat_cond_3_filtered = feat_cond_3[feat_cond_3['Percept_ID'].isin(similar_values)]\n",
    "feat_cond_4_filtered = feat_cond_4[feat_cond_4['Percept_ID'].isin(similar_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sub-analysis four conditions within subjects:\n",
    "# Add 'Condition' column to each dataframe\n",
    "feat_cond_1['Condition'] = 1\n",
    "feat_cond_2['Condition'] = 2\n",
    "feat_cond_3['Condition'] = 3\n",
    "feat_cond_4['Condition'] = 4\n",
    "trial_df = pd.concat([feat_cond_1, feat_cond_2, feat_cond_3, feat_cond_4], axis=0, ignore_index=True)\n",
    "trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 6), sharey=False)\n",
    "merged_df = trial_df\n",
    "ylabels = ['Coef. Variation. Intra Tap Int','Normalized RMS','Coef. Variation. Impact RMS','Mean Raise Velocity']\n",
    "# Iterate over columns and plot boxplots and scatters\n",
    "for i, column in enumerate(['coefVar_intraTapInt', 'trace_RMSn', 'coefVar_impactRMS', 'mean_raise_velocity']):\n",
    "    for condition in range(1, 5):  # Consider pairs of adjacent conditions\n",
    "        condition_data_curr = merged_df[merged_df['Condition'] == condition]\n",
    "        condition_data_next = merged_df[merged_df['Condition'] == condition + 1]\n",
    "\n",
    "        # Plot boxplot\n",
    "        axes[i].boxplot(condition_data_curr[column], positions=[condition], widths=0.5, showfliers=False)\n",
    "\n",
    "        # Plot scatters and connect with lines to the next condition\n",
    "        x_values_curr = [condition] * len(condition_data_curr[column])\n",
    "        x_values_next = [condition + 1] * len(condition_data_next[column])\n",
    "\n",
    "        axes[i].scatter(x_values_curr, condition_data_curr[column], color='C{}'.format(condition - 1), s=20)\n",
    "        axes[i].scatter(x_values_next, condition_data_next[column], color='C{}'.format(condition), s=20)\n",
    "\n",
    "        for x_curr, x_next, y_curr, y_next in zip(x_values_curr, x_values_next, condition_data_curr[column], condition_data_next[column]):\n",
    "            axes[i].plot([x_curr, x_next], [y_curr, y_next], color='grey'.format(condition - 1), alpha=0.3)\n",
    "\n",
    "    # Set labels and title for each subplot\n",
    "    axes[i].set_xticks([1, 2, 3, 4])\n",
    "    xticklabels = ['DBS Off', 'DBSOn-PreEntrain', 'DBSOn-FirstEtrain', 'DBSOn-HighestEntrain']\n",
    "    axes[i].set_xticklabels(xticklabels)\n",
    "    axes[i].set_xticklabels(xticklabels, rotation = 45)\n",
    "    axes[i].set_ylabel(ylabels[i])\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(project_path, 'results', 'accelerometer', 'inspections','Off-OnPreLowHigh_4conds'),dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cond_1['Cohort'] = 1\n",
    "feat_cond_2['Cohort'] = 2\n",
    "lme_dataframe = pd.concat([feat_cond_1,feat_cond_2], ignore_index=True)\n",
    "lme_dataframe.to_excel('LME_MotorPerformance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "lme_dataframe = pd.read_excel(os.path.join(project_path, 'results', 'accelerometer', 'MotorPerf_Comps','LME_MotorPerformance.xlsx'))\n",
    "lme_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = smf.mixedlm(\"mean_raise_velocity ~ Stim\", lme_dataframe, groups=lme_dataframe[\"Percept_ID\"])\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "#import mlxtend\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Get the column names excluding 'Percept_ID'\n",
    "columns = [col for col in feat_cond_1.columns if col != 'Percept_ID']\n",
    "\n",
    "#fig, axes = plt.subplots(1, len(columns), figsize=(25, 5))  # Adjust figsize as needed\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
    "axes = axes.flatten()\n",
    "# Iterate over each column and create a boxplot for each dataframe\n",
    "for i, column in enumerate(columns):\n",
    "    # Get the data for the column from each dataframe\n",
    "    data1 = feat_cond_1[column]\n",
    "    data2 = feat_cond_2[column]\n",
    "    data3 = feat_cond_3[column]\n",
    "    data4 = feat_cond_4[column]\n",
    "\n",
    "    #p_value = mlxtend.permutation_test(data1.values, data2.values, paired=False, method=\"approximate\", seed=0, num_rounds=100000)\n",
    "\n",
    "    #p_value_round = np.round(p_value, decimals = 6)\n",
    "\n",
    "    statistic, p_value = mannwhitneyu(data1, data2)\n",
    "    p_value_round = np.round(p_value, decimals = 6)\n",
    "\n",
    "\n",
    "     # Create a boxplot for the column in the corresponding subplot\n",
    "    axes[i].boxplot([data1, data2, data3, data4], widths = 0.5, showfliers=False)\n",
    "    axes[i].scatter([1]*len(data1), data1, color='dimgrey', s =20)\n",
    "    axes[i].scatter([2]*len(data2), data2, color='darkred', s=20)\n",
    "    axes[i].scatter([3]*len(data3), data3, color='green', s=20)\n",
    "    axes[i].scatter([4]*len(data4), data4, color='blue', s=20)\n",
    "\n",
    "    axes[i].set_title(column)\n",
    "\n",
    "\n",
    "    # Create a boxplot for the column in the corresponding subplot\n",
    "    #axes[i].set_title(column + ':' + str(p_value_round))\n",
    "\n",
    "# Add a legend at the bottom of the plot\n",
    "legend_labels = ['M1S0', 'M1S1:PreSubharmonic', 'M1S1:Subh-First Amp','M1S1:Subh-Highest Amp']\n",
    "legend_markers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color) for color in ['dimgrey', 'darkred', 'green','blue']]\n",
    "\n",
    "#legend_labels = ['GroupNO', 'GroupWith']\n",
    "#legend_markers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color) for color in ['dimgrey', '#DDCC77']]\n",
    "\n",
    "fig.legend(legend_markers, legend_labels, loc='lower center', ncol=3)\n",
    "\n",
    "# Adjust spacing between subplots and legend\n",
    "fig.tight_layout(rect=[0, 0.1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    json_dir, 'All_Blocks_GROUPSCOMPARISONS_WITH_WITHOUT_Wilcoxon'\n",
    "), dpi = 400\n",
    ")\n",
    "\n",
    "plt.savefig(os.path.join(\n",
    "    json_dir, 'All_Blocks_GROUPSCOMPARISONS_WITH_WITHOUT_Wilcoxon.pdf'\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ephys during movement and subharmonic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Package Loss in Json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_Name = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PERCEPT_MOTHERFOLDER\\\\RawFiles\\\\sub-028\\\\12MFU\\\\Dyskinesia\\\\Report_Json_Session_Report_20220825T131757.json'\n",
    "\n",
    "with open(file_Name, 'r') as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['BrainSenseTimeDomain'][0]['TimeDomainData'])/250/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ticks = data['BrainSenseTimeDomain'][0]['TicksInMses']\n",
    "ticks_n = np.array(re.findall(r'\\d+', ticks), dtype=int)\n",
    "print(ticks_n)\n",
    "'''diff = np.diff(ticks_n)\n",
    "indices = np.where(diff > 250)[0] + 1\n",
    "print(indices)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronize Accel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fif_path = os.path.join(project_path, 'data', 'Fifs', 'with_med_FTG')\n",
    "accel_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\raw_csv_all'\n",
    "accel_fileName = 'Sub029_M1_RampUp_250Hz_LHAND.csv'\n",
    "\n",
    "dn_accel = pd.read_csv(os.path.join(\n",
    "    accel_path,\n",
    "    accel_fileName\n",
    "), header = None\n",
    ")\n",
    "\n",
    "Draw = mne.io.read_raw_fif(os.path.join(fif_path,'Sub029_Ramp125_FIF.fif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "time_points = np.arange(dn_accel.shape[1]) / 250\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(time_points, dn_accel.iloc[i,:])\n",
    "\n",
    "# Show the modified figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of Samples of Accel data = {dn_accel.shape[1]}')\n",
    "print(f'Number of Samples of Raw data = {Draw.n_times}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_start = 34.510 *250\n",
    "num_samples_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_accel_array = dn_accel.values\n",
    "\n",
    "#num_samples_start = 23.767 *250\n",
    "#data_trimmed = dn_accel_array[:, 8627:]\n",
    "#data_trimmed.shape\n",
    "\n",
    "data_trimmed = data_trimmed[:, :-48582]\n",
    "data_trimmed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trimmed.shape[1] - Draw.n_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Draw.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = Draw.get_data(picks = 'STIM_R_125Hz_60us')[0,:]\n",
    "plt.plot(data_trimmed[0,:])\n",
    "plt.plot(stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\n",
    "    accel_path,\n",
    "    'Sub029_M1_RampUp_cut_traces.npy'\n",
    "), data_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Rest vs Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBID = 'Sub005_RampUp'\n",
    "\n",
    "#define your paths\n",
    "fif_path = os.path.join(project_path, 'data', 'Fifs', 'with_med_FTG')\n",
    "FFTs_path = os.path.join(project_path, 'data', 'FFTs', 'with_med_FTG')\n",
    "cut_traces_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\raw_csv_all\\\\Synced_traces\\\\'\n",
    "\n",
    "#Import your data\n",
    "accel = np.load(os.path.join(cut_traces_path, 'Sub005_cut_traces.npy'))\n",
    "\n",
    "#accel = np.genfromtxt(os.path.join(cut_traces_path, 'Sub005_cut_traces.csv'), delimiter=',')\n",
    "\n",
    "Draw = mne.io.read_raw_fif(os.path.join(fif_path,'Sub005_FIF.fif'))\n",
    "D = np.load(os.path.join(FFTs_path, 'sub005_FFT.npy'))\n",
    "\n",
    "Draw.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "time_points1 = np.arange(accel.shape[1]) / 250\n",
    "time_points2 = np.arange(Draw.n_times) / 250\n",
    "stim = Draw.get_data(picks = 'STIM_R_130Hz_60us')[0]\n",
    "\n",
    "plt.plot(time_points1, accel[2,:])\n",
    "plt.plot(time_points2, stim*10e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_df = pd.read_excel(os.path.join(project_path, 'documents', 'FTG_Global.xlsx'), sheet_name = 'MovementEpochs_DBSOff')\n",
    "epoch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 9\n",
    "\n",
    "rest_on = epoch_df.loc[row,'Rest_on']\n",
    "rest_off = epoch_df.loc[row,'Rest_off']\n",
    "rest_dur = rest_off - rest_on\n",
    "\n",
    "block1_on = epoch_df.loc[row,'Block1_on']\n",
    "block1_off = epoch_df.loc[row,'Block1_off']\n",
    "block1_dur = block1_off - block1_on\n",
    "\n",
    "block2_on = epoch_df.loc[row,'Block2_on']\n",
    "block2_off = epoch_df.loc[row,'Block2_off']\n",
    "block2_dur = block2_off - block2_on\n",
    "\n",
    "block3_on = epoch_df.loc[row,'Block3_on']\n",
    "block3_off = epoch_df.loc[row,'Block3_off']\n",
    "block3_dur = block3_off - block3_on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "%matplotlib qt\n",
    "\n",
    "time_points = np.arange(accel.shape[1]) / 250\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(time_points1, accel[2,:])\n",
    "plt.plot(time_points2, stim)\n",
    "ymin, ymax = plt.ylim()\n",
    "\n",
    "rect1 = patches.Rectangle((rest_on, ymin), rest_dur, ymax, linewidth=1, facecolor='red', alpha = 0.3)\n",
    "rect2 = patches.Rectangle((block1_on, ymin), block1_dur, ymax, linewidth=1, facecolor='turquoise', alpha = 0.3)\n",
    "rect3 = patches.Rectangle((block2_on, ymin), block2_dur, ymax, linewidth=1, facecolor='turquoise', alpha = 0.3)\n",
    "rect4 = patches.Rectangle((block3_on, ymin), block3_dur, ymax, linewidth=1, facecolor='turquoise', alpha = 0.3)\n",
    "\n",
    "plt.gca().add_patch(rect1)\n",
    "plt.gca().add_patch(rect2)\n",
    "plt.gca().add_patch(rect3)\n",
    "plt.gca().add_patch(rect4)\n",
    "\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Acceleration')\n",
    "\n",
    "plt.title(f'{SUBID}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', 'STIM_OFF', f'{SUBID}_Epochs.jpg'),\n",
    "    dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIDE = 1\n",
    "\n",
    "rest_ps = np.nanmean(D[SIDE, :, int(rest_on):int(rest_off)], 1)\n",
    "\n",
    "tap1_ps = np.nanmean(D[SIDE, :, int(block1_on):int(block1_off)], 1)\n",
    "tap2_ps = np.nanmean(D[SIDE, :, int(block2_on):int(block2_off)], 1)\n",
    "tap3_ps = np.nanmean(D[SIDE, :, int(block3_on):int(block3_off)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,127), rest_ps, label = 'Rest', lw = 2, linestyle = 'dashed')\n",
    "plt.plot(np.arange(1,127), tap1_ps, label = 'TapBlock1', lw = 2)\n",
    "plt.plot(np.arange(1,127), tap2_ps, label = 'TapBlock2', lw = 2)\n",
    "plt.plot(np.arange(1,127), tap3_ps, label = 'TapBlock3', lw = 2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(40,90)\n",
    "plt.ylim(0,0.2)\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('LFP Power')\n",
    "\n",
    "plt.title(f'{SUBID}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul','STIM_OFF', f'{SUBID}_PowerSpectra.jpg'),\n",
    "    dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_array = np.array([tap1_ps, tap2_ps, tap3_ps])\n",
    "tap_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(project_path, 'data','movement_modul', 'STIM_OFF',f'{SUBID}_RestPS.npy'), rest_ps)\n",
    "np.save(os.path.join(project_path, 'data','movement_modul', 'STIM_OFF', f'{SUBID}_TapPS.npy'), tap_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average all taps and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "data_dir = os.path.join(project_path, 'data', 'movement_modul', 'STIM_OFF\\\\')\n",
    "\n",
    "file_list = glob.glob(data_dir + '*TapPS.npy')\n",
    "\n",
    "# Initialize an empty list to store the arrays\n",
    "for file_path in file_list:\n",
    "    array = np.load(file_path)\n",
    "    averaged_array = np.mean(array, axis=0)\n",
    "    print(averaged_array.shape)\n",
    "    \n",
    "    name = file_path.split('\\\\')[-1].split('_')[0]\n",
    "    print(name)\n",
    "\n",
    "    np.save(os.path.join(\n",
    "        data_dir, f'{name}_AVG_TAP.npy'\n",
    "    ), averaged_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "subject_names = set()\n",
    "data_dir = os.path.join(project_path, 'data', 'movement_modul', 'STIM_OFF\\\\')\n",
    "# Collect subject names\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".npy\"):\n",
    "        subject_name = filename.split(\"_\")[0]\n",
    "        subject_names.add(subject_name)\n",
    "\n",
    "\n",
    "all_rest = []\n",
    "all_tap = []\n",
    "\n",
    "# Load and plot matching pairs\n",
    "for subject_name in sorted(subject_names):\n",
    "    tap_file = os.path.join(data_dir, f\"{subject_name}_AVG_TAP.npy\")\n",
    "    rest_file = os.path.join(data_dir, f\"{subject_name}_RestPs.npy\")\n",
    "\n",
    "    if os.path.isfile(tap_file) and os.path.isfile(rest_file):\n",
    "        tap_data = np.load(tap_file)\n",
    "        rest_data = np.load(rest_file)\n",
    "        print(subject_name)\n",
    "        all_rest.append(rest_data)\n",
    "        all_tap.append(tap_data)\n",
    "        # Plotting code\n",
    "        plt.figure()\n",
    "        plt.plot(tap_data, label=\"Tap\")\n",
    "        plt.plot(rest_data, label=\"Rest\")\n",
    "        plt.title(f\"Subject: {subject_name}\")\n",
    "        #plt.ylim(0,0.4)\n",
    "        plt.xlim(5,40)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        '''plt.savefig(os.path.join(\n",
    "            data_dir,\n",
    "            f\"{subject_name}_MeanMod\"\n",
    "        ), dpi = 200)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rest_df = np.transpose(np.array(all_rest))\n",
    "all_rest_df = pd.DataFrame(all_rest_df)\n",
    "all_rest_df.columns = (sorted(subject_names))\n",
    "all_rest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tap_df = np.transpose(np.array(all_tap))\n",
    "all_tap_df = pd.DataFrame(all_tap_df)\n",
    "all_tap_df.columns = (sorted(subject_names))\n",
    "all_tap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(project_path, 'data', 'movement_modul', 'STIM_OFF\\\\')\n",
    "#all_rest_df = pd.read_excel(os.path.join(data_dir,'AllRest_preTap.xlsx'),index_col=None)\n",
    "#all_tap_df = pd.read_excel(os.path.join(data_dir,'AVG_TAP.xlsx'),index_col=None)\n",
    "#all_tap_df\n",
    "all_rest_df.to_excel(os.path.join(data_dir,'AllRest_preTap.xlsx'))\n",
    "all_tap_df.to_excel(os.path.join(data_dir,'AVG_TAP.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colDict = {}\n",
    "for key in ['Sub005', 'Sub007']:\n",
    "    colDict[key] = 64\n",
    "for key in ['Sub021','Sub025','Sub028','Sub033','Sub050','Sub065']:\n",
    "    colDict[key] = 62\n",
    "for key in ['Sub029']:\n",
    "    colDict[key] = 72\n",
    "\n",
    "max_d = {'Percept_ID':all_tap_df.columns,\n",
    "          'Max_Rest_Subh': max_values_df1,\n",
    "          'Max_Tap_Subh': max_values_df2\n",
    "}\n",
    "\n",
    "max_df = pd.DataFrame(data = max_d)\n",
    "max_df\n",
    "#statistic, p_value = mannwhitneyu(max_values_df1, max_values_df2)\n",
    "#p_value_round = np.round(p_value, decimals = 6)\n",
    "\n",
    "#print(p_value_round)\n",
    "\n",
    "max_df.to_excel(os.path.join(project_path,'data','movement_modul', 'MaxSubh_RestMove.xlsx'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_rest_gamma = all_rest_df.iloc[50:81]\n",
    "#all_tap_gamma = all_tap_df.iloc[50:81]\n",
    "\n",
    "#rest_smoothed = all_rest_gamma.rolling(window=3, min_periods=1).mean()\n",
    "#tap_smoothed = all_tap_gamma.rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "#max_values_df1 = all_rest_gamma.max()\n",
    "df2_normalized = all_tap_df.div(max_df['Max_Rest_Subh'].values)\n",
    "df2_normalized.columns = all_tap_df.columns\n",
    "df2_normalized.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to add NaN rows and shift values\n",
    "columns_63 = ['Sub021', 'Sub025', 'Sub028','Sub033', 'Sub050', 'Sub065']\n",
    "columns_73 = ['Sub029']\n",
    "\n",
    "df2_normalized2 = df2_normalized\n",
    "# Adding NaN rows and shifting values for specified columns\n",
    "for column in columns_63:\n",
    "    # Shift values down by the number of rows\n",
    "    df2_normalized2[column] = df2_normalized[column].shift(2)\n",
    "\n",
    "    # Add NaN rows to the beginning of the column\n",
    "    df2_normalized.loc[:2-1, column] = None\n",
    "\n",
    "for column1 in columns_73:\n",
    "    df2_normalized2[column1] = df2_normalized2[column1].shift(-7)\n",
    "    #df2_normalized2.loc[:10-1, column1] = None\n",
    "\n",
    "\n",
    "df2_normalized2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "mean_tap = np.nanmean(df2_normalized2, axis = 1)\n",
    "sem_tap = stats.sem(df2_normalized2, axis = 1, nan_policy= 'omit')\n",
    "\n",
    "plt.plot(np.arange(1,127), mean_tap, color = 'purple', lw = 2)\n",
    "plt.plot(np.arange(1,127), df2_normalized2, color = 'purple', alpha = 0.18)\n",
    "plt.fill_between(np.arange(1,127),mean_tap - sem_tap, mean_tap + sem_tap, color = 'purple', alpha=0.3, linestyle = 'dashed', lw = 1.5)\n",
    "plt.axhline(y = 1, color = 'grey', linestyle = ':', alpha = 0.5)\n",
    "plt.xlim(50,80)\n",
    "plt.ylim(0,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_columns_with_transparency(dataframe):\n",
    "    # Line color and transparency\n",
    "    transparencies = np.linspace(0.3,1,9)\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot columns with increasing transparency\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "            dataframe[column].plot(ax=ax, color = 'purple', alpha=transparencies[i], label=f'{column}', lw = 2.5)\n",
    "\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_ylabel('LFP Power [Normalized to Rest]')\n",
    "    # Add a legend\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Plot the DataFrame columns\n",
    "plot_columns_with_transparency(df2_normalized)\n",
    "plt.axhline(y = 1, color = 'grey', linestyle = ':', alpha = 0.3)\n",
    "plt.xlim(50,80)\n",
    "plt.xticks(np.arange(50,85,5), labels = ['-15','-10','-5','1:2\\nEntrainment', '+5','+10','+15'])\n",
    "plt.ylim(0,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', 'Movement_Modulation_All'),\n",
    "    dpi=200)\n",
    "\n",
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', 'Movement_Modulation_All.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots Gamma Sync Stim Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.join(\n",
    "    project_path, 'data', 'movement_modul', 'STIM_OFF'\n",
    ")\n",
    "\n",
    "all_rest_df = pd.read_excel(os.path.join(dir, 'AllRest_preTap.xlsx'), index_col=None)\n",
    "all_tap_df = pd.read_excel(os.path.join(dir, 'AVG_TAP.xlsx'), index_col=None)\n",
    "\n",
    "mean_rest = all_rest_df.iloc[39:89].mean(axis = 0)\n",
    "mean_tap = all_tap_df.iloc[39:89].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample array of 9 strings\n",
    "labels_array = list(all_tap_df.columns.values)\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot([mean_rest, mean_tap], widths=0.3, showfliers=False, labels=['Rest', 'Tap'])\n",
    "\n",
    "# Define a colormap with 9 distinct colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 9))\n",
    "\n",
    "# Scatter plot for mean_rest and mean_tap with unique colors and labels\n",
    "for i in range(len(mean_rest)):\n",
    "    ax.scatter([1, 2], [mean_rest[i], mean_tap[i]], color=colors[i], s=60, label=labels_array[i])\n",
    "\n",
    "# Connecting lines\n",
    "for x1, x2, y1, y2 in zip([1] * len(mean_rest), [2] * len(mean_tap), mean_rest, mean_tap):\n",
    "    plt.plot([x1, x2], [y1, y2], 'k:', alpha=0.3)\n",
    "\n",
    "plt.ylabel('Mean LFP Power 40-90Hz')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks([1, 2], ['Rest', 'Tap'])\n",
    "plt.title('Gamma Modulation Movement Med On-Stim Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    dir,'MoveMod_StimOff'\n",
    "), dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary Patient Movement Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_npy = np.load(os.path.join(\n",
    "    project_path,\n",
    "    'data', 'FFTs', 'with_med_FTG',\n",
    "    'Sub029_FFT.npy'\n",
    "))\n",
    "\n",
    "fft_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel = np.load('S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\raw_csv_all\\\\Synced_traces\\\\Sub029_cut_traces.npy')\n",
    "accel.shape\n",
    "import dat_preproc\n",
    "faccel = dat_preproc.low_highpass_filter(accel, 1, 124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = np.full((1,323),1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax2 = ax[0].twinx()\n",
    "time_points = np.arange(accel.shape[1]) / 250\n",
    "\n",
    "ax[0].pcolormesh(fft_npy[1,:,:], cmap = 'viridis', vmin = 0, vmax = 0.8) #vmin = -4, vmax = 7)\n",
    "ax[0].set_xlim(100,200)\n",
    "ax[0].set_ylim(60,80)\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_yticklabels([])\n",
    "ax2.plot(np.arange(0, fft_npy.shape[2]), stim[0], color = 'white', linewidth = 2.5, linestyle = 'dotted')\n",
    "ax2.set_yticks(np.arange(1,3,0.5))\n",
    "ax2.set_yticklabels([])\n",
    "ax[1].plot(time_points, faccel[2,:], color = 'grey', alpha = 0.8)\n",
    "ax[1].set_xlim(100,200)\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "#plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', 'Sub029_ExemplaryMovementEmpty'),\n",
    "    dpi=200)\n",
    "\n",
    "'''plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', 'Sub029_ExemplaryMovement.jpg'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import mne  # Import the mne module if not already imported\n",
    "\n",
    "def save_pdf_without_antialiasing(file_path, fig, dpi=100):\n",
    "    with PdfPages(file_path) as pdf:\n",
    "        # Save the figure as a PDF without anti-aliasing\n",
    "        pdf.savefig(fig, dpi=dpi)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    # Replace 'figure.pdf' with the desired file path and name for your PDF file\n",
    "    file_path = \"figure.pdf\"\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8.5, 11))\n",
    "    bs_data = mne.baseline.rescale(data=fft_npy[1, :, :], times=np.arange(0, fft_npy.shape[2]), baseline=baseline, mode='zscore')\n",
    "\n",
    "    ax[0].pcolormesh(fft_npy[1, :, :], cmap='viridis', vmin=0, vmax=0.8)\n",
    "    ax[0].set_xlim(100, 200)\n",
    "    ax[0].set_ylim(60, 80)\n",
    "    ax[0].set_ylabel('Frequency [Hz]')\n",
    "\n",
    "    ax[1].plot(time_points, faccel[2, :], color='grey', alpha=0.8)\n",
    "    ax[1].set_xlim(100, 200)\n",
    "    ax[1].set_ylabel('Acceleration [g]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "\n",
    "    save_pdf_without_antialiasing(file_path, fig)\n",
    "\n",
    "    # Show the plot (optional)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor Performance with Increased Stim Amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write down in Excel all epochs and amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corresponding files\n",
    "SUBID = 'Sub065'\n",
    "\n",
    "#define your paths\n",
    "fif_path = os.path.join(project_path, 'data', 'Fifs', 'with_med_FTG')\n",
    "FFTs_path = os.path.join(project_path, 'data', 'FFTs', 'with_med_FTG')\n",
    "cut_traces_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\raw_csv_all\\\\Synced_traces\\\\'\n",
    "\n",
    "#Import your data\n",
    "accel = np.load(os.path.join(cut_traces_path, 'Sub065_cut_traces.npy'))\n",
    "#accel = np.genfromtxt(os.path.join(cut_traces_path, 'Sub050_cut_traces.csv'), delimiter=',')\n",
    "\n",
    "Draw = mne.io.read_raw_fif(os.path.join(fif_path,'Sub065_FIF.fif'))\n",
    "\n",
    "Draw.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "time_points1 = np.arange(accel.shape[1]) / 250\n",
    "time_points2 = np.arange(Draw.n_times) / 250\n",
    "stim = Draw.get_data(picks = 'STIM_L_125Hz_60us')[0]\n",
    "\n",
    "plt.plot(time_points1, accel[2,:])\n",
    "plt.plot(time_points2, stim) #*10e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify all Features from blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBID = 'Sub033'\n",
    "\n",
    "blocks_dir = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\retap_results\\\\features\\\\'\n",
    "#blocks_dir = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\without_med_FTG\\\\raw_files\\\\retap_results\\\\features'\n",
    "blocks_epochs = pd.read_excel(os.path.join(\n",
    "    project_path, 'results', 'accelerometer', 'Accel_Blocks_AllInBetween.xlsx'\n",
    "))\n",
    "\n",
    "all_files = os.listdir(blocks_dir)\n",
    "\n",
    "json_files = [file for file in all_files if file.startswith(f'features_{SUBID}') and file.endswith('.json')]\n",
    "\n",
    "for json_file in json_files:\n",
    "    print(os.path.join(blocks_dir, json_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Average the Blocks with the same Amp together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = blocks_epochs[blocks_epochs['PerceptID'] == SUBID].dropna(subset=['Block'])\n",
    "filtered_df['Block'] = filtered_df['Block'].astype('int')\n",
    "blocks_with_same_amplitude = filtered_df.groupby('Amplitude')['Block'].unique().dropna().to_dict()\n",
    "\n",
    "keys_to_average = ['coefVar_intraTapInt', 'trace_RMSn', 'coefVar_impactRMS', 'mean_raise_velocity']\n",
    "\n",
    "\n",
    "for amplitude, blocks in blocks_with_same_amplitude.items():\n",
    "    amplitude_blocks = [f'block{block}' for block in blocks]\n",
    "    entrainment_value = filtered_df[filtered_df['Block'].isin(blocks)]['Entrainment'].tolist()[0]\n",
    "    # Filter files that have the same block number\n",
    "    relevant_files = [file for file in json_files if any(f'block{block}.json' in file for block in blocks)]\n",
    "\n",
    "    averaged_values = {'SubID': SUBID, 'amplitude': amplitude, 'entrainment': entrainment_value}\n",
    "\n",
    "    for key in keys_to_average:\n",
    "        values = []\n",
    "\n",
    "        # Extract values for the key from each relevant file\n",
    "        for file in relevant_files:\n",
    "            try:\n",
    "                with open(os.path.join(blocks_dir, file), 'r') as f:\n",
    "                    file_data = json.load(f)\n",
    "                    if key in file_data:\n",
    "                        values.append(file_data[key])\n",
    "                    else:\n",
    "                        values.append(None)  # Handle the case where the key is missing\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "        # Calculate the average for the key\n",
    "        if values:\n",
    "            averaged_values[key] = sum(values) / len(values)\n",
    "        else:\n",
    "            averaged_values[key] = None  \n",
    "                \n",
    "        amp_nondot = str(amplitude).replace('.', '')\n",
    "        features_json_title = f'{SUBID}_AvgFeatures_Amp{amp_nondot}mA.json'\n",
    "        json_avg_features = os.path.join(\n",
    "            project_path, 'results', 'accelerometer', 'Avg_Features_allBlocks',\n",
    "            features_json_title\n",
    "        )\n",
    "\n",
    "        with open(json_avg_features, 'w') as json_file:\n",
    "            json.dump(averaged_values, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot all together and inspect the effect of Entrainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_features = os.path.join(project_path, 'results', 'accelerometer', 'Avg_Features_allBlocks')\n",
    "all_jsons_avg_blocks = os.listdir(avg_features)\n",
    "json_allblocks_thisSUB = [file for file in all_jsons_avg_blocks if file.startswith(SUBID) and file.endswith('.json')]\n",
    "json_allblocks_thisSUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example key of interest\n",
    "keys_of_interest = ['coefVar_intraTapInt', 'trace_RMSn', 'coefVar_impactRMS', 'mean_raise_velocity']\n",
    "\n",
    "# Initialize a figure with subplots\n",
    "fig, axs = plt.subplots(len(keys_of_interest), 1, figsize=(8, 2 * len(keys_of_interest)), sharex=True)\n",
    "\n",
    "data_dict = {'SubID': SUBID}\n",
    "\n",
    "# Iterate through each key of interest\n",
    "for i, key_of_interest in enumerate(keys_of_interest):\n",
    "    amplitudes = []\n",
    "    values_of_interest = []\n",
    "    entrainments = []\n",
    "    colors = []\n",
    "\n",
    "    # Iterate through each JSON file\n",
    "    for json_file in json_allblocks_thisSUB:\n",
    "        with open(os.path.join(avg_features, json_file), 'r') as f:\n",
    "            file_data = json.load(f)\n",
    "\n",
    "            # Check if the key of interest is present in the dictionary\n",
    "            if key_of_interest in file_data:\n",
    "                # Extract 'amplitude' value and the value of the key of interest\n",
    "                amplitude = file_data.get('amplitude', None)\n",
    "                value_of_interest = file_data[key_of_interest]\n",
    "                entrainment = file_data.get('entrainment')\n",
    "\n",
    "                # Append values to the lists\n",
    "                amplitudes.append(amplitude)\n",
    "                values_of_interest.append(value_of_interest)\n",
    "                entrainments.append(entrainment)\n",
    "                # Set color based on 'entrainment' value\n",
    "                colors.append('red' if entrainment == 1 else 'blue')\n",
    "\n",
    "    # Create a subplot for each key of interest\n",
    "    axs[i].plot(amplitudes, values_of_interest, color='gray', linestyle='-', label='Values')\n",
    "    axs[i].scatter(amplitudes, values_of_interest, c=colors, label='Values')\n",
    "    axs[i].set_ylabel(f'{key_of_interest} Values')\n",
    "    axs[i].set_ylabel(f'{key_of_interest}')\n",
    "    \n",
    "    # Create a dictionary for each key of interest\n",
    "    key_data_dict = {\n",
    "        'amplitudes': amplitudes,\n",
    "        'values_of_interest': values_of_interest,\n",
    "        'entrainments': entrainments\n",
    "    }\n",
    "\n",
    "    # Add the nested dictionary to the main dictionary\n",
    "    data_dict[key_of_interest] = key_data_dict\n",
    "\n",
    "axs[0].set_title(SUBID)\n",
    "axs[-1].set_xlabel('Amplitude (mA)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### SAVE ALL\n",
    "avg_features = os.path.join(project_path, 'results', 'accelerometer', 'Avg_Features_allBlocks','All_Amplitudes')\n",
    "json_all_avg = f'{SUBID}_AvgFeatures_AllAmps.json'\n",
    "json_filename = os.path.join(avg_features,json_all_avg)\n",
    "with open(json_filename, 'w') as json_file:\n",
    "            json.dump(data_dict, json_file)\n",
    "\n",
    "plt.savefig(os.path.join(avg_features,f'{SUBID}__AvgFeatures_AllAmps'), dpi = 150)\n",
    "\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Stimulation Intensity\n",
    "\n",
    "amps_excel = pd.read_excel(os.path.join(project_path,'results','accelerometer','Accel_Blocks_AllInBetween.xlsx'),\n",
    "                           sheet_name='Amplitudes')\n",
    "amps_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to normalize\n",
    "columns_to_normalize = amps_excel.columns[1:8]\n",
    "\n",
    "# Copy the DataFrame to avoid modifying the original\n",
    "normalized_dataframe = amps_excel.copy()\n",
    "\n",
    "# Iterate through each row and normalize the values\n",
    "for index, row in normalized_dataframe.iterrows():\n",
    "    for column in columns_to_normalize:\n",
    "        # Calculate the percentage difference with respect to the value in 'On_Entrain' column\n",
    "        normalized_value = (row[column] / row['On_Entrain']) * 100\n",
    "        normalized_dataframe.at[index, column] = normalized_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "avg_features = os.path.join(project_path, 'results', 'accelerometer', 'Avg_Features_allBlocks','All_Amplitudes')\n",
    "\n",
    "features = ['coefVar_intraTapInt', 'trace_RMSn', 'coefVar_impactRMS', 'mean_raise_velocity']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(8, 2 * 4), sharex=True)\n",
    "\n",
    "for jk, this_feat in enumerate(features):\n",
    "\n",
    "    data_sets = []\n",
    "    \n",
    "    for subID in normalized_dataframe['SubID']:\n",
    "        SUBID = subID\n",
    "\n",
    "        this_file = f'{SUBID}_AvgFeatures_AllAmps.json'\n",
    "\n",
    "        json_file_path = os.path.join(avg_features, this_file)\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        idx_row = normalized_dataframe[normalized_dataframe['SubID'] == SUBID].index[0]\n",
    "        norm_amps = normalized_dataframe.iloc[idx_row,1:7].dropna().tolist()\n",
    "\n",
    "        motor_feat = np.array(data[this_feat]['values_of_interest'])\n",
    "        #motor_feat = ((motor_feat - motor_feat[0]) / motor_feat[0]) * 100\n",
    "        \n",
    "        this_data_set = {'SubID': SUBID, 'norm_amps': norm_amps, 'motor_feat': motor_feat}\n",
    "        data_sets.append(this_data_set)\n",
    "\n",
    "        ### SAVE ALL\n",
    "        json_datraset_allfeats = f'AllFeatDBSIntensity_{this_feat}.pkl'\n",
    "        json_filename = os.path.join(avg_features,json_datraset_allfeats)\n",
    "        with open(json_filename, 'wb') as f:\n",
    "           pickle.dump(data_sets, f)\n",
    "        \n",
    "        '''for i, data in enumerate(data_sets):\n",
    "            axs[jk].plot(data['norm_amps'], data['motor_feat'], label=f'{SUBID}', marker='o', linestyle='-')\n",
    "            axs[jk].axvline(x=100, lw = 0.3, ls = ':', color = 'grey')\n",
    "            axs[jk].set_ylabel(f'{this_feat}\\n(% Change DBS Off)')\n",
    "            axs[jk].set_xlabel('DBS Intensity [reltive to entrainment amplitude %]')\n",
    "            #axs[jk].set_xticklabels(np.arange(-20, 160, 20))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(avg_features,'AllFeatures_Plot'), dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## MOTOR FEATURES RELATIVE TO DBS OFF ##############\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "avg_features = os.path.join(project_path, 'results', 'accelerometer', 'Avg_Features_allBlocks','All_Amplitudes')\n",
    "\n",
    "pickle_filenames = ['AllFeatDBSIntensity_mean_raise_velocity.pkl',\n",
    "                    'AllFeatDBSIntensity_trace_RMSn.pkl',\n",
    "                    'AllFeatDBSIntensity_coefVar_impactRMS.pkl',\n",
    "                    'AllFeatDBSIntensity_coefVar_intraTapInt.pkl']\n",
    "\n",
    "feature_names = ['Mean Raise Velocity [m/s]',\n",
    "                 'Norm. Trace RMS',\n",
    "                 'Coef. Var. Impact RMS',\n",
    "                 'Coef. Var. Intra Tap Interval']\n",
    "\n",
    "###### baseline correction function ######\n",
    "def baseline_correct(group):\n",
    "    baseline_value = group.loc[group['motor_feat_range'] == RANGE, 'motor_feat'].values\n",
    "    group['motor_feat'] = (group['motor_feat'] - baseline_value) / baseline_value * 100\n",
    "    return group\n",
    "##########################################\n",
    "\n",
    "RANGE = 0 #condition (see bins below) relative to which baseline correction will be applied\n",
    "\n",
    "if RANGE == 0:\n",
    "    ylabel = '(% Relative to DBS-Off)'\n",
    "elif RANGE == 2:\n",
    "    ylabel = '(% Relative to Pre-Entrain)'\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(7, 9.5))\n",
    "\n",
    "for kl, pkl_filename in enumerate(pickle_filenames):\n",
    "    file_path = os.path.join(avg_features,pickle_filenames[kl])\n",
    "    with open(file_path, 'rb') as f:\n",
    "        my_dict = pickle.load(f)\n",
    "\n",
    "    # Unpack the dictionaries\n",
    "    norm_amps = []\n",
    "    motor_feat = []\n",
    "    sub_ids = []\n",
    "    for entry in my_dict:\n",
    "        sub_id = entry['SubID']\n",
    "        for amp, feat in zip(entry['norm_amps'], entry['motor_feat']):\n",
    "            norm_amps.append(amp)\n",
    "            motor_feat.append(feat)\n",
    "            sub_ids.append(sub_id)\n",
    "\n",
    "    df_unpacked = pd.DataFrame({'SubID': sub_ids, 'norm_amp': norm_amps, 'motor_feat': motor_feat})\n",
    "\n",
    "    # Correct the normalized amplitudes into bins & average\n",
    "    bins = [0, 20, 60, 90, 101, float('inf')]\n",
    "    labels = [0,1,2,3,4]\n",
    "    df_unpacked['motor_feat_range'] = pd.cut(df_unpacked['norm_amp'], bins=bins, labels=labels, include_lowest=True)\n",
    "    df_unpacked_avg = df_unpacked.groupby(['SubID', 'motor_feat_range']).agg({'norm_amp': 'mean', 'motor_feat': 'mean'}).reset_index()\n",
    "    df_unpacked_avg['motor_feat_change'] = df_unpacked_avg.groupby('SubID')['motor_feat'].pct_change()\n",
    "    df_unpacked_avg = df_unpacked_avg.dropna(subset=['motor_feat_change'])\n",
    "    \n",
    "    # Apply baseline correction for each subject\n",
    "    #baseline_corrected_df = df_unpacked_avg.groupby('SubID').apply(baseline_correct)\n",
    "    \n",
    "    # Boxplots    \n",
    "    DATAFRAME = df_unpacked_avg\n",
    "    \n",
    "    ax = axes[kl]  # Select the appropriate subplot\n",
    "    y = 'motor_feat_change'\n",
    "    boxplot = sns.boxplot(x='motor_feat_range', y=y, data=DATAFRAME, \n",
    "                          width=0.5, showfliers=False, color='white', ax=ax)\n",
    "    colors = sns.color_palette('viridis', len(DATAFRAME['SubID'].unique()))\n",
    "\n",
    "    # Adjust marker size for scatter plot\n",
    "    for sub_id, color in zip(DATAFRAME['SubID'].unique(), colors):\n",
    "        sub_data = DATAFRAME[DATAFRAME['SubID'] == sub_id]\n",
    "        sub_data_sorted = sub_data.sort_values(by='motor_feat_range')\n",
    "        ax.scatter(sub_data_sorted['motor_feat_range'], sub_data_sorted[y], color=color, label=sub_id, s=40)  # Increase marker size\n",
    "\n",
    "    #ax.set_xlabel('DBS Intensity (% relative to Amp of Entrainment Onset)')\n",
    "    #ax.set_xticklabels(['0% (DBS Off)','25-60%','60-90% \\n (Pre-Entrain)','100% \\n (Entrain Onset)','>100%'])\n",
    "    ax.set_xticklabels(['0% (DBS Off)','25-60%','60-90% \\n (Pre-Entrain)','100% \\n (Entrain Onset)','>100%'])\n",
    "    ax.set_ylabel(f'{feature_names[kl]} \\n {ylabel}')\n",
    "    #ax.legend()\n",
    "\n",
    "    # Set x-label for the last subplot\n",
    "    if kl == len(pickle_filenames) - 1:\n",
    "        ax.set_xlabel('DBS Intensity \\n (% relative to Amp of Entrainment Onset)', labelpad=20)\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figname = 'All_features_RelativeOff'\n",
    "plt.savefig(os.path.join(avg_features,figname), dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## MOTOR FEATURES RELATIVE TO PRE-ENTRAIN ##############\n",
    "\n",
    "def baseline_correctpreEntrain(group):\n",
    "    baseline_value = None\n",
    "    \n",
    "    # Check for motor_feat_range == 2\n",
    "    if (group['motor_feat_range'] == 2).any():\n",
    "        non_nan_values = group.loc[group['motor_feat_range'] == 2, 'motor_feat'].dropna()\n",
    "        if not non_nan_values.empty:\n",
    "            baseline_value = non_nan_values.values[0]\n",
    "\n",
    "    # Check for motor_feat_range == 1 if baseline_value is still None\n",
    "    if baseline_value is None and (group['motor_feat_range'] == 1).any():\n",
    "        non_nan_values = group.loc[group['motor_feat_range'] == 1, 'motor_feat'].dropna()\n",
    "        if not non_nan_values.empty:\n",
    "            baseline_value = non_nan_values.values[0]\n",
    "    \n",
    "    if baseline_value is not None:\n",
    "        group['motor_feat_Corr'] = (group['motor_feat'] - baseline_value) / baseline_value * 100\n",
    "    else:\n",
    "        group['motor_feat_Corr'] = np.nan  # Mark all values as NaN\n",
    "        \n",
    "    return group\n",
    "\n",
    "avg_features = os.path.join(project_path, 'results', 'accelerometer', 'Avg_Features_allBlocks','All_Amplitudes')\n",
    "\n",
    "pickle_filenames = ['AllFeatDBSIntensity_mean_raise_velocity.pkl',\n",
    "                    'AllFeatDBSIntensity_trace_RMSn.pkl',\n",
    "                    'AllFeatDBSIntensity_coefVar_impactRMS.pkl',\n",
    "                    'AllFeatDBSIntensity_coefVar_intraTapInt.pkl']\n",
    "\n",
    "feature_names = ['Mean Raise Velocity',\n",
    "                 'Norm. Trace RMS',\n",
    "                 'Coef. Var. Impact RMS',\n",
    "                 'Coef. Var. Intra Tap Interval']\n",
    "\n",
    "fig, axes = plt.subplots(1,4, figsize=(15,5))\n",
    "\n",
    "for kl, pkl_filename in enumerate(pickle_filenames):\n",
    "    file_path = os.path.join(avg_features,pickle_filenames[kl])\n",
    "    with open(file_path, 'rb') as f:\n",
    "        my_dict = pickle.load(f)\n",
    "\n",
    "    # Unpack the dictionaries\n",
    "    norm_amps = []\n",
    "    motor_feat = []\n",
    "    sub_ids = []\n",
    "    for entry in my_dict:\n",
    "        sub_id = entry['SubID']\n",
    "        for amp, feat in zip(entry['norm_amps'], entry['motor_feat']):\n",
    "            norm_amps.append(amp)\n",
    "            motor_feat.append(feat)\n",
    "            sub_ids.append(sub_id)\n",
    "\n",
    "    df_unpacked = pd.DataFrame({'SubID': sub_ids, 'norm_amp': norm_amps, 'motor_feat': motor_feat})\n",
    "\n",
    "    # Correct the normalized amplitudes into bins & average\n",
    "    bins = [0, 20, 60, 90, 101, float('inf')]\n",
    "    labels = [0,1,2,3,4]\n",
    "    df_unpacked['motor_feat_range'] = pd.cut(df_unpacked['norm_amp'], bins=bins, labels=labels, include_lowest=True)\n",
    "    #df_unpacked_avg = df_unpacked.groupby(['SubID', 'motor_feat_range']).agg({'norm_amp': 'mean', 'motor_feat': 'mean'}).reset_index()\n",
    "    \n",
    "    baseline_correctedPreEntrain_df = df_unpacked.groupby('SubID').apply(baseline_correctpreEntrain)\n",
    "    \n",
    "    string_without_gaps = feature_names[kl].replace(\" \", \"\")\n",
    "    #baseline_correctedPreEntrain_df.to_excel(os.path.join(\n",
    "    #    avg_features, f'{string_without_gaps}.xlsx'\n",
    "    #), index=None)\n",
    "    \n",
    "    plot_entr = baseline_correctedPreEntrain_df['motor_feat_Corr'][baseline_correctedPreEntrain_df['norm_amp'] == 100.0].dropna()\n",
    "    \n",
    "    ax = axes[kl]\n",
    "    ax.boxplot(plot_entr)\n",
    "    ax.scatter(np.ones((1, len(plot_entr))),plot_entr)\n",
    "    ax.axhline(y=0, color='grey', linestyle=':')\n",
    "    ax.set_xticklabels(['Stim Amp of Entrainment Onset'])\n",
    "    ax.set_title(f'{feature_names[kl]} \\n % Rel. Stim. Amplitude \\n Pre Entrain')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figname = 'All_features_RelativePreEntrainment'\n",
    "plt.savefig(os.path.join(avg_features,figname), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyskinesia Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dysk_ratings = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Ratings'\n",
    "\n",
    "\n",
    "for filename in os.listdir(path_dysk_ratings):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        \n",
    "\n",
    "        rate_df = pd.read_excel(os.path.join(\n",
    "            path_dysk_ratings, filename\n",
    "        ), sheet_name='Dataset')\n",
    "        rate_df['Ipsi_Delta'] = rate_df['Ipsi'] - rate_df['Ipsi'][0]\n",
    "        rate_df['Contra_Delta'] = rate_df['Contra'] - rate_df['Contra'][0]\n",
    "        rate_df['Total_Delta'] = rate_df['Total'] - rate_df['Total'][0]\n",
    "        \n",
    "        SUBID = filename.split('.')[0]\n",
    "        \n",
    "        %matplotlib qt\n",
    "\n",
    "        fig, axes = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "        if any(rate_df['Reporting'] == 1):\n",
    "            side_eff_onset1 = rate_df.loc[rate_df['Reporting'] == 1, 'Amplitude'].tolist()[0]\n",
    "            side_eff_onset2 = rate_df['Amplitude_Norm'][rate_df['Reporting'] == 1].tolist()[0]\n",
    "            \n",
    "        axes[0].scatter(rate_df['Amplitude'], rate_df['Contra'], marker='o', label = 'Contralateral CDRS')\n",
    "        axes[0].scatter(rate_df['Amplitude'], rate_df['Ipsi'], label = 'Ipsilateral CDRS')\n",
    "        axes[0].scatter(rate_df['Amplitude'], rate_df['Total'], label = 'Total CDRS')\n",
    "        axes[0].set_xlabel('DBS Amplitude (mA)')\n",
    "        axes[0].set_ylabel('CDRS Score')\n",
    "        axes[0].axvline(entrain_onset1, color = 'grey', ls = ':', label = 'Entrainment Onset')\n",
    "        axes[0].axvline(side_eff_onset1, color = 'magenta', ls = ':', label = 'Side-Effect Reporting')\n",
    "        axes[0].legend()\n",
    "\n",
    "\n",
    "        entrain_onset2 = rate_df['Amplitude_Norm'][rate_df['Amplitude_Norm'] == 100].tolist()[0]\n",
    "        axes[1].scatter(rate_df['Amplitude_Norm'], rate_df['Contra_Delta'], marker='o', label = 'Contralateral CDRS')\n",
    "        axes[1].scatter(rate_df['Amplitude_Norm'], rate_df['Ipsi_Delta'], label = 'Ipsilateral CDRS')\n",
    "        axes[1].scatter(rate_df['Amplitude_Norm'], rate_df['Total_Delta'], label = 'Total CDRS')\n",
    "        axes[1].set_xlabel('DBS Intensity (% Rel to Amp of Entrainment Onset)')\n",
    "        axes[1].set_ylabel('CDRS Score (Delta Rel to DBS Off)')\n",
    "        axes[1].axvline(entrain_onset2, color = 'grey', ls = ':', label = 'Entrainment Onset')\n",
    "        axes[1].axvline(side_eff_onset2, color = 'magenta', ls = ':', label = 'Side-Effect Reporting')\n",
    "        axes[1].legend()\n",
    "\n",
    "        plt.suptitle(SUBID)\n",
    "        \n",
    "        plt.savefig(os.path.join(\n",
    "            project_path, 'figures', 'CDRS', f'{SUBID}_CDRS'\n",
    "        ), dpi = 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group Figure\n",
    "import random\n",
    "path_dysk_ratings = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Ratings'\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8,6))\n",
    "\n",
    "dict_cohorts = {\n",
    "    'cohort_with' : ['Sub005', 'Sub006','Sub009','Sub014','Sub017','Sub029','Sub045','Sub065'],\n",
    "    'cohort_without' : ['Sub007', 'Sub021','Sub025','Sub028','Sub033','Sub043','Sub050']\n",
    "}\n",
    "\n",
    "lines_with = []\n",
    "lines_without = []\n",
    "entrain_lines = []\n",
    "\n",
    "for filename in os.listdir(path_dysk_ratings):\n",
    "    random_number = random.uniform(-0.7, 0)\n",
    "    if filename.endswith('.xlsx'):\n",
    "        rate_df = pd.read_excel(os.path.join(path_dysk_ratings, filename), sheet_name='Dataset')\n",
    "        #rate_df['Ipsi_Delta'] = rate_df['Ipsi'] - rate_df['Ipsi'][0]\n",
    "        #rate_df['Contra_Delta'] = rate_df['Contra'] - rate_df['Contra'][0]\n",
    "        #rate_df['Total_Delta'] = rate_df['Total'] - rate_df['Total'][0]\n",
    "        if rate_df['Total'][0] == 0:\n",
    "            rate_df['Total'] = rate_df['Total'] + random_number\n",
    "        SUBID = filename.split('.')[0]\n",
    "        \n",
    "        rate_df['Amp_Dev'] = rate_df['Amplitude'] - rate_df.loc[rate_df['Amplitude_Norm'] == 100, 'Amplitude'].values[0]\n",
    "        #rate_df.loc[rate_df['Amplitude_Norm'] == 0, 'Amp_Dev'] = 0\n",
    "\n",
    "        entrain_onset2 = rate_df['Amp_Dev'][rate_df['Amp_Dev'] == 0].tolist()[0]\n",
    "        entrain_line = plt.axvline(entrain_onset2, color='grey', ls=':', label = 'Entrainment Onset')\n",
    "        if SUBID in dict_cohorts['cohort_with']:\n",
    "            color='#88CCEE'\n",
    "            line, = plt.plot(rate_df['Amp_Dev'], rate_df['Total'], color=color, label='with FTG DBS-Off', lw = 2)\n",
    "            lines_with.append(line)\n",
    "            entrain_lines.append(entrain_line)\n",
    "        elif SUBID in dict_cohorts['cohort_without']:\n",
    "            color='#AA4499' \n",
    "            line, = plt.plot(rate_df['Amp_Dev'], rate_df['Total'], color=color,label='no FTG DBS-Off', lw = 2)\n",
    "            lines_without.append(line)\n",
    "        \n",
    "        \n",
    "    \n",
    "axes.legend(handles=[lines_with[0], lines_without[0], entrain_line], labels=['with FTG DBS-Off (n=8)', 'no FTG DBS-Off (n=7)','Entrainment Onset'])\n",
    "plt.xlabel('DBS Intensity (mA Rel to Amp of Entrainment Onset)')\n",
    "plt.ylabel('Total CDRS Score')\n",
    "#axes.set_xticklabels(['','0% (DBS-Off)','50%','100% \\n Entrain Onset','150%','200%','250%'])\n",
    "axes.set_yticks(np.arange(0,13,2))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "            project_path, 'figures', 'CDRS', 'Total_CDRS_GroupFigure_Relative'\n",
    "        ), dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
