{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Directories/ Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import find_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedrive = find_folders.get_onedrive_path()\n",
    "project_path = find_folders.get_onedrive_path(\"entrainment\")\n",
    "print(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_blocks_df = pd.read_excel(os.path.join(\n",
    "    project_path,\n",
    "    'results',\n",
    "    'accelerometer',\n",
    "    'Accel_Blocks.xlsx'\n",
    "))\n",
    "\n",
    "accel_blocks_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\retap_results\\\\features'\n",
    "blocks_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\retap_results\\\\extracted_tapblocks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features = ['freq', 'mean_raise_velocity', 'coefVar_raise_velocity', 'trace_RMSn', 'coefVar_intraTapInt', 'slope_intraTapInt','trace_entropy','jerkiness_trace', 'mean_tapRMS',\n",
    "                'coefVar_tapRMS', 'mean_impactRMS', 'coefVar_impactRMS', 'slope_impactRMS', 'coefVar_tap_entropy', 'slope_tap_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs = accel_blocks_df['Percept_ID'].unique()\n",
    "\n",
    "for sub in all_subs:\n",
    "    print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in all_subs:\n",
    "\n",
    "    this_sub = sub\n",
    "\n",
    "    for cond in np.arange(1,5):\n",
    "\n",
    "        this_cond = cond\n",
    "\n",
    "        these_blocks = accel_blocks_df.loc[(accel_blocks_df['Percept_ID'] == this_sub) & (accel_blocks_df['Cond'] == this_cond), 'Blocks']\n",
    "        block_strings = ['block' + str(num) for num in these_blocks]\n",
    "        print(block_strings)\n",
    "\n",
    "        # Filter the json files based on conditions\n",
    "        jsons_to_import = []\n",
    "        for filename in os.listdir(features_path):\n",
    "            if filename.endswith('.json') and this_sub in filename:\n",
    "                for block_str in block_strings:\n",
    "                    if filename.endswith(f'{block_str}.json'):\n",
    "                        jsons_to_import.append(filename)\n",
    "\n",
    "        print(jsons_to_import)\n",
    "\n",
    "        # Filter the csv files based on conditions\n",
    "        csvs_to_import = []\n",
    "        for filename in os.listdir(blocks_path):\n",
    "            if filename.endswith('.csv') and this_sub in filename:\n",
    "                for block_str in block_strings:\n",
    "                    if filename.endswith(f'{block_str}_250Hz.csv'):\n",
    "                        csvs_to_import.append(filename)\n",
    "\n",
    "        print(csvs_to_import)\n",
    "\n",
    "        # Read the selected JSON files\n",
    "\n",
    "        combined_dict = []\n",
    "        for filename in jsons_to_import:\n",
    "            file_path = os.path.join(features_path, filename)\n",
    "            with open(file_path) as f:\n",
    "                this_block_feat = json.load(f)\n",
    "                matching_keys = set(imp_features) & set(this_block_feat.keys())\n",
    "                combined_dict.append({key: this_block_feat[key] for key in matching_keys})\n",
    "\n",
    "        #SAVE IT\n",
    "        suptitle = str(this_sub) + ' - Condition ' + str(this_cond)\n",
    "\n",
    "        file_name = \"\".join(suptitle.split()) + '_features.json'\n",
    "\n",
    "        # Combine the file name with the current directory to create the file path\n",
    "        file_path = os.path.join(\n",
    "            project_path,\n",
    "            'results',\n",
    "            'accelerometer',\n",
    "            'inspections')\n",
    "\n",
    "        # Serializing json\n",
    "        json_object = json.dumps(combined_dict, indent=4)\n",
    "        \n",
    "        # Writing to sample.json\n",
    "        with open(os.path.join(file_path,file_name), \"w\") as outfile:\n",
    "            outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, axs = plt.subplots(len(csvs_to_import),1, figsize = (18,10))\n",
    "\n",
    "suptitle = str(this_sub) + ' - Condition ' + str(this_cond)\n",
    "fig.suptitle(suptitle, fontsize=14, fontweight='bold')\n",
    "\n",
    "rounded_list = [\n",
    "    {key: round(value, 2) for key, value in dictionary.items()}\n",
    "    for dictionary in combined_dict\n",
    "]\n",
    "\n",
    "for num, file in enumerate(csvs_to_import):\n",
    "    this_block_csv = pd.read_csv(os.path.join(blocks_path,file))\n",
    "    axs[num].plot(this_block_csv)\n",
    "    axs[num].set_ylabel('Acceleration [g] - ' + str(block_strings[num]))\n",
    "    axs[num].set_title(rounded_list[num])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'results',\n",
    "    'accelerometer',\n",
    "    'inspections',\n",
    "    \"\".join(suptitle.split()),\n",
    "), dpi = 250\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average all blocks for each condition within subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = os.path.join(\n",
    "    project_path,\n",
    "    'results', 'accelerometer', 'inspections', 'Condition4'\n",
    ")\n",
    "all_features_cond1 = pd.DataFrame()\n",
    "\n",
    "# Loop through the JSON files in the directory\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        percept_id = filename[:6]\n",
    "\n",
    "        # Load the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # Initialize a dictionary to store the accumulated sums and counts for each key\n",
    "        summed_values = {}\n",
    "        count_values = {}\n",
    "        \n",
    "        # Iterate through the dictionaries in the JSON data\n",
    "        for data_dict in json_data:\n",
    "            # Accumulate the sums and counts for each key\n",
    "            for key, value in data_dict.items():\n",
    "                if key not in summed_values:\n",
    "                    summed_values[key] = value\n",
    "                    count_values[key] = 1\n",
    "                else:\n",
    "                    summed_values[key] += value\n",
    "                    count_values[key] += 1\n",
    "        \n",
    "        # Calculate the averages for each key\n",
    "        averaged_values = {key: summed_values[key] / count_values[key] for key in summed_values}\n",
    "        averaged_values['Percept_ID'] = percept_id\n",
    "        \n",
    "        # Append the averaged values to the dataframe\n",
    "        all_features_cond1 = all_features_cond1.append(averaged_values, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting dataframe\n",
    "all_features_cond1 = all_features_cond1.reindex(columns=['Percept_ID'] + list(all_features_cond1.columns[:-1]))\n",
    "print(all_features_cond1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as JSON\n",
    "json_file_path = os.path.join(json_dir, 'all_features_cond4.json')\n",
    "all_features_cond1.to_json(json_file_path, orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all averaged values in three conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = os.path.join(\n",
    "    project_path,\n",
    "    'results', 'accelerometer', 'inspections\\\\'\n",
    ")\n",
    "\n",
    "feat_cond_1 = pd.read_json(str(json_dir) + 'all_features_cond1.json')\n",
    "feat_cond_2 = pd.read_json(str(json_dir) + 'all_features_cond2.json')\n",
    "feat_cond_4 = pd.read_json(str(json_dir) + 'all_features_cond4.json')\n",
    "\n",
    "feat_cond_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column values as NumPy arrays\n",
    "column1_values = feat_cond_1['Percept_ID'].values\n",
    "column2_values = feat_cond_2['Percept_ID'].values\n",
    "column3_values = feat_cond_4['Percept_ID'].values\n",
    "\n",
    "# Find the similar values between the three columns\n",
    "similar_values = np.intersect1d(column1_values, np.intersect1d(column2_values, column3_values))\n",
    "print(similar_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "# Get the column names excluding 'Percept_ID'\n",
    "columns = [col for col in feat_cond_1.columns if col != 'Percept_ID']\n",
    "\n",
    "#fig, axes = plt.subplots(1, len(columns), figsize=(25, 5))  # Adjust figsize as needed\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "# Iterate over each column and create a boxplot for each dataframe\n",
    "for i, column in enumerate(columns):\n",
    "    # Get the data for the column from each dataframe\n",
    "    data1 = feat_cond_1[column]\n",
    "    data2 = feat_cond_2[column]\n",
    "    data3 = feat_cond_3[column]\n",
    "\n",
    "     # Create a boxplot for the column in the corresponding subplot\n",
    "    axes[i].boxplot([data1, data2, data3], showfliers=False)\n",
    "    axes[i].scatter([1]*len(data1), data1, color='red')\n",
    "    axes[i].scatter([2]*len(data2), data2, color='blue')\n",
    "    axes[i].scatter([3]*len(data3), data3, color='green')\n",
    "    axes[i].set_title(column)\n",
    "\n",
    "\n",
    "    # Create a boxplot for the column in the corresponding subplot\n",
    "    axes[i].boxplot([data1, data2, data3])\n",
    "    axes[i].set_title(column)\n",
    "\n",
    "# Add a legend at the bottom of the plot\n",
    "legend_labels = ['M1S0', 'M1S1:PreSubharmonic', 'M1S1:Subh-Highest Amp']\n",
    "legend_markers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color) for color in ['red', 'blue', 'green']]\n",
    "fig.legend(legend_markers, legend_labels, loc='lower center', ncol=3)\n",
    "\n",
    "# Adjust spacing between subplots and legend\n",
    "fig.tight_layout(rect=[0, 0.1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    json_dir, 'All_avg_features_conds'\n",
    "), dpi = 250\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare motor performance in patients with vs without subharmonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
