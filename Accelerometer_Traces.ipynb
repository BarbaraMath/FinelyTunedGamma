{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Directories/ Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import find_folders\n",
    "import mne\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedrive = find_folders.get_onedrive_path()\n",
    "project_path = find_folders.get_onedrive_path(\"entrainment\")\n",
    "print(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw_accel = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\subcohort_noENTRAINMENT\\\\raw'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle TMSI Accelerometer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CODE\n",
    "import tmsi_poly5reader\n",
    "\n",
    "poly5_name = 'Sub020_12mfu_M1S0_BStr_RSTN_FTRamp-20220509T133001.DATA.Poly5'\n",
    "\n",
    "# LOAD FILE\n",
    "accel = tmsi_poly5reader.Poly5Reader(\n",
    "    os.path.join(path_raw_accel,\n",
    "                 poly5_name)\n",
    ")\n",
    "\n",
    "raw_acc = accel.read_data_MNE()\n",
    "dn_accel = raw_acc.resample(250, npad=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_accel.ch_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dn_accel = dn_accel.get_data(picks=[3,4,5])\n",
    "my_dn_accel.shape\n",
    "plt.plot(my_dn_accel[2,:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(\n",
    "    path_raw_accel,\n",
    "    'Sub020_M1_250Hz_LHAND.csv'\n",
    "), my_dn_accel, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_blocks_df = pd.read_excel(os.path.join(\n",
    "    project_path,\n",
    "    'results',\n",
    "    'accelerometer',\n",
    "    'Accel_Blocks_NoEntrainment.xlsx'\n",
    "))\n",
    "\n",
    "accel_blocks_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\subcohort_noENTRAINMENT\\\\retap_results\\\\features'\n",
    "blocks_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\subcohort_noENTRAINMENT\\\\retap_results\\\\extracted_tapblocks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features = ['freq', 'mean_raise_velocity', 'coefVar_raise_velocity', 'trace_RMSn', 'coefVar_intraTapInt', 'slope_intraTapInt','trace_entropy','jerkiness_trace', 'mean_tapRMS',\n",
    "                'coefVar_tapRMS', 'mean_impactRMS', 'coefVar_impactRMS', 'slope_impactRMS', 'coefVar_tap_entropy', 'slope_tap_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs = accel_blocks_df['Percept_ID'].unique()\n",
    "\n",
    "for sub in all_subs:\n",
    "    print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [1,4]\n",
    "\n",
    "for sub in all_subs:\n",
    "\n",
    "    this_sub = sub\n",
    "\n",
    "    for cond in conditions:\n",
    "\n",
    "        this_cond = cond\n",
    "\n",
    "        these_blocks = accel_blocks_df.loc[(accel_blocks_df['Percept_ID'] == this_sub) & (accel_blocks_df['Cond'] == this_cond), 'Blocks']\n",
    "        block_strings = ['block' + str(num) for num in these_blocks]\n",
    "        print(block_strings)\n",
    "\n",
    "        # Filter the json files based on conditions\n",
    "        jsons_to_import = []\n",
    "        for filename in os.listdir(features_path):\n",
    "            if filename.endswith('.json') and this_sub in filename:\n",
    "                for block_str in block_strings:\n",
    "                    if filename.endswith(f'{block_str}.json'):\n",
    "                        jsons_to_import.append(filename)\n",
    "\n",
    "        print(jsons_to_import)\n",
    "\n",
    "        # Filter the csv files based on conditions\n",
    "        csvs_to_import = []\n",
    "        for filename in os.listdir(blocks_path):\n",
    "            if filename.endswith('.csv') and this_sub in filename:\n",
    "                for block_str in block_strings:\n",
    "                    if filename.endswith(f'{block_str}_250Hz.csv'):\n",
    "                        csvs_to_import.append(filename)\n",
    "\n",
    "        print(csvs_to_import)\n",
    "\n",
    "        # Read the selected JSON files\n",
    "\n",
    "        combined_dict = []\n",
    "        for filename in jsons_to_import:\n",
    "            file_path = os.path.join(features_path, filename)\n",
    "            with open(file_path) as f:\n",
    "                this_block_feat = json.load(f)\n",
    "                matching_keys = set(imp_features) & set(this_block_feat.keys())\n",
    "                combined_dict.append({key: this_block_feat[key] for key in matching_keys})\n",
    "\n",
    "        #SAVE IT\n",
    "        suptitle = str(this_sub) + ' - Condition ' + str(this_cond)\n",
    "\n",
    "        file_name = \"\".join(suptitle.split()) + '_features.json'\n",
    "\n",
    "        #PLOT IT\n",
    "        %matplotlib qt\n",
    "        fig, axs = plt.subplots(len(csvs_to_import),1, figsize = (18,10))\n",
    "\n",
    "        suptitle = str(this_sub) + ' - Condition ' + str(this_cond)\n",
    "        fig.suptitle(suptitle, fontsize=14, fontweight='bold')\n",
    "\n",
    "        rounded_list = [\n",
    "            {key: round(value, 2) for key, value in dictionary.items()}\n",
    "            for dictionary in combined_dict\n",
    "        ]\n",
    "\n",
    "        for num, file in enumerate(csvs_to_import):\n",
    "            this_block_csv = pd.read_csv(os.path.join(blocks_path,file))\n",
    "            axs[num].plot(this_block_csv)\n",
    "            axs[num].set_ylabel('Acceleration [g] - ' + str(block_strings[num]))\n",
    "            axs[num].set_title(rounded_list[num])\n",
    "\n",
    "            plt.savefig(os.path.join(\n",
    "            project_path,\n",
    "            'results',\n",
    "            'accelerometer',\n",
    "            'inspections',\n",
    "            'group_NO_Entrainment',\n",
    "            \"\".join(suptitle.split()),\n",
    "            ), dpi = 250\n",
    "            )\n",
    "                \n",
    "            # Combine the file name with the current directory to create the file path\n",
    "            file_path = os.path.join(\n",
    "                project_path,\n",
    "                'results',\n",
    "                'accelerometer',\n",
    "                'inspections',\n",
    "                'group_NO_Entrainment')\n",
    "\n",
    "            # Serializing json\n",
    "            json_object = json.dumps(combined_dict, indent=4)\n",
    "            \n",
    "            # Writing to sample.json\n",
    "            with open(os.path.join(file_path,file_name), \"w\") as outfile:\n",
    "                outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, axs = plt.subplots(len(csvs_to_import),1, figsize = (18,10))\n",
    "\n",
    "suptitle = str(this_sub) + ' - Condition ' + str(this_cond)\n",
    "fig.suptitle(suptitle, fontsize=14, fontweight='bold')\n",
    "\n",
    "rounded_list = [\n",
    "    {key: round(value, 2) for key, value in dictionary.items()}\n",
    "    for dictionary in combined_dict\n",
    "]\n",
    "\n",
    "for num, file in enumerate(csvs_to_import):\n",
    "    this_block_csv = pd.read_csv(os.path.join(blocks_path,file))\n",
    "    axs[num].plot(this_block_csv)\n",
    "    axs[num].set_ylabel('Acceleration [g] - ' + str(block_strings[num]))\n",
    "    axs[num].set_title(rounded_list[num])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'results',\n",
    "    'accelerometer',\n",
    "    'inspections',\n",
    "    \"\".join(suptitle.split()),\n",
    "), dpi = 250\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average all blocks for each condition within subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = os.path.join(\n",
    "    project_path,\n",
    "    'results', 'accelerometer', 'inspections', 'group_NO_Entrainment', 'Condition_4'\n",
    ")\n",
    "\n",
    "all_features_cond1 = pd.DataFrame()\n",
    "\n",
    "# Loop through the JSON files in the directory\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        percept_id = filename[:6]\n",
    "\n",
    "        # Load the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # Initialize a dictionary to store the accumulated sums and counts for each key\n",
    "        summed_values = {}\n",
    "        count_values = {}\n",
    "        \n",
    "        # Iterate through the dictionaries in the JSON data\n",
    "        for data_dict in json_data:\n",
    "            # Accumulate the sums and counts for each key\n",
    "            for key, value in data_dict.items():\n",
    "                if key not in summed_values:\n",
    "                    summed_values[key] = value\n",
    "                    count_values[key] = 1\n",
    "                else:\n",
    "                    summed_values[key] += value\n",
    "                    count_values[key] += 1\n",
    "        \n",
    "        # Calculate the averages for each key\n",
    "        #averaged_values = {key: summed_values[key] / count_values[key] for key in summed_values}\n",
    "        averaged_values = {key: summed_values[key] for key in summed_values}\n",
    "        averaged_values['Percept_ID'] = percept_id\n",
    "        \n",
    "        # Append the averaged values to the dataframe\n",
    "        all_features_cond1 = all_features_cond1.append(averaged_values, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting dataframe\n",
    "all_features_cond1 = all_features_cond1.reindex(columns=['Percept_ID'] + list(all_features_cond1.columns[:-1]))\n",
    "print(all_features_cond1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as JSON\n",
    "json_file_path = os.path.join(json_dir, 'all_featuresNOENTR_cond4.json')\n",
    "all_features_cond1.to_json(json_file_path, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all blocks together without averaging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = os.path.join(project_path, 'results', 'accelerometer', 'MotorPerf_Comps')\n",
    "\n",
    "filenames = os.listdir(json_dir)\n",
    "json_filenames = [filename for filename in filenames if not filename.startswith('.DS_Store')]\n",
    "\n",
    "data_frames = []\n",
    "for filename in json_filenames:\n",
    "    file_path = os.path.join(json_dir, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "        df.insert(0, 'Percept_ID',filename[:6])   # Extract filename without extension\n",
    "        data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single big DataFrame\n",
    "big_dataframe = pd.concat(data_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = os.path.join(json_dir, 'all_featureswithENTR_cond4_ALLBLOCKS.json')\n",
    "big_dataframe.to_json(json_file_path, orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all averaged values in three conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''json_dir = os.path.join(\n",
    "    project_path,\n",
    "    'results', 'accelerometer', 'inspections\\\\'\n",
    ")'''\n",
    "\n",
    "json_dir = os.path.join(project_path, 'results', 'accelerometer', 'MotorPerf_Comps')\n",
    "\n",
    "feat_cond_1 = pd.read_json(os.path.join(json_dir,'all_featuresNOENTR_cond4_ALLBLOCKS.json'))\n",
    "feat_cond_2 = pd.read_json(os.path.join(json_dir,'all_featureswithENTR_cond4_ALLBLOCKS.json'))\n",
    "\n",
    "fts_of_interest = ['Percept_ID',\"coefVar_intraTapInt\", \"trace_RMSn\", \"coefVar_impactRMS\", \"mean_raise_velocity\"]\n",
    "feat_cond_1 = feat_cond_1[fts_of_interest]\n",
    "feat_cond_2 = feat_cond_2[fts_of_interest]\n",
    "\n",
    "#feat_cond_4 = pd.read_json(str(json_dir) + 'all_features_cond4.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column values as NumPy arrays\n",
    "column1_values = feat_cond_1['Percept_ID'].values\n",
    "column2_values = feat_cond_2['Percept_ID'].values\n",
    "#column3_values = feat_cond_4['Percept_ID'].values\n",
    "\n",
    "# Find the similar values between the three columns\n",
    "similar_values = np.intersect1d(column1_values, np.intersect1d(column2_values, column3_values))\n",
    "print(similar_values)\n",
    "\n",
    "# Keep only the rows with similar values in each dataframe\n",
    "feat_cond_1_filtered = feat_cond_1[feat_cond_1['Percept_ID'].isin(similar_values)]\n",
    "feat_cond_2_filtered = feat_cond_2[feat_cond_2['Percept_ID'].isin(similar_values)]\n",
    "#feat_cond_4_filtered = feat_cond_4[feat_cond_4['Percept_ID'].isin(similar_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cond_1['Cohort'] = 1\n",
    "feat_cond_2['Cohort'] = 2\n",
    "lme_dataframe = pd.concat([feat_cond_1,feat_cond_2], ignore_index=True)\n",
    "lme_dataframe.to_excel('LME_MotorPerformance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "lme_dataframe = pd.read_excel(os.path.join(project_path, 'results', 'accelerometer', 'MotorPerf_Comps','LME_MotorPerformance.xlsx'))\n",
    "lme_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = smf.mixedlm(\"mean_raise_velocity ~ Stim\", lme_dataframe, groups=lme_dataframe[\"Percept_ID\"])\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "#import mlxtend\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Get the column names excluding 'Percept_ID'\n",
    "columns = [col for col in feat_cond_1.columns if col != 'Percept_ID']\n",
    "\n",
    "#fig, axes = plt.subplots(1, len(columns), figsize=(25, 5))  # Adjust figsize as needed\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
    "axes = axes.flatten()\n",
    "# Iterate over each column and create a boxplot for each dataframe\n",
    "for i, column in enumerate(columns):\n",
    "    # Get the data for the column from each dataframe\n",
    "    data1 = feat_cond_1[column]\n",
    "    data2 = feat_cond_2[column]\n",
    "    #data3 = feat_cond_4[column]\n",
    "\n",
    "    #p_value = mlxtend.permutation_test(data1.values, data2.values, paired=False, method=\"approximate\", seed=0, num_rounds=100000)\n",
    "\n",
    "    #p_value_round = np.round(p_value, decimals = 6)\n",
    "\n",
    "    statistic, p_value = mannwhitneyu(data1, data2)\n",
    "    p_value_round = np.round(p_value, decimals = 6)\n",
    "\n",
    "\n",
    "     # Create a boxplot for the column in the corresponding subplot\n",
    "    axes[i].boxplot([data1, data2], widths = 0.5, showfliers=False)\n",
    "    axes[i].scatter([1]*len(data1), data1, color='dimgrey', s =20)\n",
    "    axes[i].scatter([2]*len(data2), data2, color='darkred', s=20)\n",
    "    #axes[i].scatter([3]*len(data3), data3, color='green')\n",
    "    axes[i].set_title(column)\n",
    "\n",
    "\n",
    "    # Create a boxplot for the column in the corresponding subplot\n",
    "    axes[i].set_title(column + ':' + str(p_value_round))\n",
    "\n",
    "# Add a legend at the bottom of the plot\n",
    "#legend_labels = ['M1S0', 'M1S1:PreSubharmonic', 'M1S1:Subh-Highest Amp']\n",
    "#legend_markers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color) for color in ['red', 'blue', 'green']]\n",
    "\n",
    "legend_labels = ['GroupNO', 'GroupWith']\n",
    "legend_markers = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color) for color in ['dimgrey', '#DDCC77']]\n",
    "\n",
    "fig.legend(legend_markers, legend_labels, loc='lower center', ncol=3)\n",
    "\n",
    "# Adjust spacing between subplots and legend\n",
    "fig.tight_layout(rect=[0, 0.1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    json_dir, 'All_Blocks_GROUPSCOMPARISONS_WITH_WITHOUT_Wilcoxon'\n",
    "), dpi = 400\n",
    ")\n",
    "\n",
    "plt.savefig(os.path.join(\n",
    "    json_dir, 'All_Blocks_GROUPSCOMPARISONS_WITH_WITHOUT_Wilcoxon.pdf'\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ephys during movement and subharmonic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Package Loss in Json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_Name = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PERCEPT_MOTHERFOLDER\\\\RawFiles\\\\sub-028\\\\12MFU\\\\Dyskinesia\\\\Report_Json_Session_Report_20220825T131757.json'\n",
    "\n",
    "with open(file_Name, 'r') as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['BrainSenseTimeDomain'][0]['TimeDomainData'])/250/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ticks = data['BrainSenseTimeDomain'][0]['TicksInMses']\n",
    "ticks_n = np.array(re.findall(r'\\d+', ticks), dtype=int)\n",
    "print(ticks_n)\n",
    "'''diff = np.diff(ticks_n)\n",
    "indices = np.where(diff > 250)[0] + 1\n",
    "print(indices)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronize Accel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fif_path = os.path.join(project_path, 'data', 'Fifs', 'with_med_FTG')\n",
    "accel_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\raw_csv_all'\n",
    "accel_fileName = 'Sub029_M1_250Hz_LHAND.csv'\n",
    "\n",
    "dn_accel = pd.read_csv(os.path.join(\n",
    "    accel_path,\n",
    "    accel_fileName\n",
    "), header = None\n",
    ")\n",
    "\n",
    "Draw = mne.io.read_raw_fif(os.path.join(fif_path,'Sub029_FIF.fif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "time_points = np.arange(dn_accel.shape[1]) / 250\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(time_points, dn_accel.iloc[i,:])\n",
    "\n",
    "# Show the modified figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of Samples of Accel data = {dn_accel.shape[1]}')\n",
    "print(f'Number of Samples of Raw data = {Draw.n_times}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_start = 23.767 *250\n",
    "num_samples_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_accel_array = dn_accel.values\n",
    "\n",
    "num_samples_start = 23.767 *250\n",
    "data_trimmed = dn_accel_array[:, 5942:]\n",
    "data_trimmed.shape\n",
    "\n",
    "#data_trimmed = data_trimmed[:, :-6781]\n",
    "data_trimmed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = Draw.get_data(picks = 'STIM_R_145Hz_60')[0,:]\n",
    "plt.plot(data_trimmed[0,:])\n",
    "plt.plot(stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\n",
    "    accel_path,\n",
    "    'Sub029_cut_traces.npy'\n",
    "), data_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Rest vs Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBID = 'Sub025'\n",
    "\n",
    "#define your paths\n",
    "fif_path = os.path.join(project_path, 'data', 'Fifs', 'without_med_FTG')\n",
    "FFTs_path = os.path.join(project_path, 'data', 'FFTs', 'without_med_FTG')\n",
    "cut_traces_path = 'S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\raw_csv_all\\\\Synced_traces'\n",
    "\n",
    "#Import your data\n",
    "accel = np.load(os.path.join(cut_traces_path, 'Sub025_cut_traces.npy'))\n",
    "\n",
    "#accel = np.genfromtxt(os.path.join(cut_traces_path, 'Sub033_cut_traces.csv'), delimiter=',')\n",
    "\n",
    "Draw = mne.io.read_raw_fif(os.path.join(fif_path,'Sub025_Connected_.fif'))\n",
    "D = np.load(os.path.join(FFTs_path, f'{SUBID}_FFT_Connected.npy'))\n",
    "\n",
    "Draw.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "time_points1 = np.arange(accel.shape[1]) / 250\n",
    "time_points2 = np.arange(Draw.n_times) / 250\n",
    "stim = Draw.get_data(picks = 'STIM_R_125Hz_60us')[0]\n",
    "\n",
    "plt.plot(time_points1, accel[2,:])\n",
    "plt.plot(time_points2, stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_df = pd.read_excel(os.path.join(project_path, 'documents', 'FTG_Global.xlsx'), sheet_name = 'Rec_Description')\n",
    "epoch_df.iloc[:, [1] + list(range(11, 20))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 7\n",
    "\n",
    "rest_on = epoch_df.loc[row,'Rest_on']\n",
    "rest_off = epoch_df.loc[row,'Rest_off']\n",
    "rest_dur = rest_off - rest_on\n",
    "\n",
    "block1_on = epoch_df.loc[row,'Block1_on']\n",
    "block1_off = epoch_df.loc[row,'Block1_off']\n",
    "block1_dur = block1_off - block1_on\n",
    "\n",
    "block2_on = epoch_df.loc[row,'Block2_on']\n",
    "block2_off = epoch_df.loc[row,'Block2_off']\n",
    "block2_dur = block2_off - block2_on\n",
    "\n",
    "block3_on = epoch_df.loc[row,'Block3_on']\n",
    "block3_off = epoch_df.loc[row,'Block3_off']\n",
    "block3_dur = block3_off - block3_on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "%matplotlib qt\n",
    "\n",
    "time_points = np.arange(accel.shape[1]) / 250\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(time_points1, accel[2,:])\n",
    "plt.plot(time_points2, stim)\n",
    "ymin, ymax = plt.ylim()\n",
    "\n",
    "rect1 = patches.Rectangle((rest_on, ymin), rest_dur, ymax, linewidth=1, facecolor='red', alpha = 0.3)\n",
    "rect2 = patches.Rectangle((block1_on, ymin), block1_dur, ymax, linewidth=1, facecolor='turquoise', alpha = 0.3)\n",
    "rect3 = patches.Rectangle((block2_on, ymin), block2_dur, ymax, linewidth=1, facecolor='turquoise', alpha = 0.3)\n",
    "rect4 = patches.Rectangle((block3_on, ymin), block3_dur, ymax, linewidth=1, facecolor='turquoise', alpha = 0.3)\n",
    "\n",
    "plt.gca().add_patch(rect1)\n",
    "plt.gca().add_patch(rect2)\n",
    "plt.gca().add_patch(rect3)\n",
    "plt.gca().add_patch(rect4)\n",
    "\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Acceleration')\n",
    "\n",
    "plt.title(f'{SUBID}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', f'{SUBID}_Epochs.jpg'),\n",
    "    dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIDE = 1\n",
    "\n",
    "rest_ps = np.nanmean(D[SIDE, :, int(rest_on):int(rest_off)], 1)\n",
    "\n",
    "tap1_ps = np.nanmean(D[SIDE, :, int(block1_on):int(block1_off)], 1)\n",
    "tap2_ps = np.nanmean(D[SIDE, :, int(block2_on):int(block2_off)], 1)\n",
    "tap3_ps = np.nanmean(D[SIDE, :, int(block3_on):int(block3_off)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,127), rest_ps, label = 'Rest', lw = 2, linestyle = 'dashed')\n",
    "plt.plot(np.arange(1,127), tap1_ps, label = 'TapBlock1', lw = 2)\n",
    "plt.plot(np.arange(1,127), tap2_ps, label = 'TapBlock2', lw = 2)\n",
    "plt.plot(np.arange(1,127), tap3_ps, label = 'TapBlock3', lw = 2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(40,90)\n",
    "plt.ylim(0,0.1)\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('LFP Power')\n",
    "\n",
    "plt.title(f'{SUBID}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', f'{SUBID}_PowerSpectra.jpg'),\n",
    "    dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tap_array = np.array([tap1_ps, tap2_ps, tap3_ps])\n",
    "tap_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(project_path, 'data','movement_modul', f'{SUBID}_RestPS.npy'), rest_ps)\n",
    "np.save(os.path.join(project_path, 'data','movement_modul', f'{SUBID}_TapPS.npy'), tap_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average all taps and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "data_dir = os.path.join(project_path, 'data', 'movement_modul')\n",
    "\n",
    "file_list = glob.glob(data_dir + '*TapPS.npy')\n",
    "\n",
    "# Initialize an empty list to store the arrays\n",
    "for file_path in file_list:\n",
    "    array = np.load(file_path)\n",
    "    print(array)\n",
    "    break\n",
    "    averaged_array = np.mean(array, axis=0)\n",
    "    print(averaged_array.shape)\n",
    "    \n",
    "    name = file_path.split('\\\\')[-1].split('_')[0]\n",
    "    print(name)\n",
    "\n",
    "    np.save(os.path.join(\n",
    "        data_dir, f'{name}_AVG_TAP.npy'\n",
    "    ), averaged_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "subject_names = set()\n",
    "data_dir = os.path.join(project_path, 'data', 'movement_modul')\n",
    "# Collect subject names\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".npy\"):\n",
    "        subject_name = filename.split(\"_\")[0]\n",
    "        subject_names.add(subject_name)\n",
    "\n",
    "\n",
    "all_rest = []\n",
    "all_tap = []\n",
    "\n",
    "# Load and plot matching pairs\n",
    "for subject_name in sorted(subject_names):\n",
    "    tap_file = os.path.join(data_dir, f\"{subject_name}_AVG_TAP.npy\")\n",
    "    rest_file = os.path.join(data_dir, f\"{subject_name}_RestPs.npy\")\n",
    "\n",
    "    if os.path.isfile(tap_file) and os.path.isfile(rest_file):\n",
    "        tap_data = np.load(tap_file)\n",
    "        rest_data = np.load(rest_file)\n",
    "        print(subject_name)\n",
    "        all_rest.append(rest_data)\n",
    "        all_tap.append(tap_data)\n",
    "        # Plotting code\n",
    "        plt.figure()\n",
    "        plt.plot(tap_data, label=\"Tap\")\n",
    "        plt.plot(rest_data, label=\"Rest\")\n",
    "        plt.title(f\"Subject: {subject_name}\")\n",
    "        plt.ylim(0,0.4)\n",
    "        plt.xlim(50,70)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rest_df = np.transpose(np.array(all_rest))\n",
    "all_rest_df = pd.DataFrame(all_rest_df)\n",
    "all_rest_df.columns = (sorted(subject_names))\n",
    "all_rest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tap_df = np.transpose(np.array(all_tap))\n",
    "all_tap_df = pd.DataFrame(all_tap_df)\n",
    "all_tap_df.columns = (sorted(subject_names))\n",
    "all_tap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(project_path, 'data', 'movement_modul\\\\')\n",
    "all_rest_df = pd.read_excel(os.path.join(data_dir,'AllRest_preTap.xlsx'),index_col=None)\n",
    "all_tap_df = pd.read_excel(os.path.join(data_dir,'AVG_TAP.xlsx'),index_col=None)\n",
    "all_tap_df\n",
    "#all_rest_df.to_excel(os.path.join(data_dir,'AllRest_preTap.xlsx'))\n",
    "#all_tap_df.to_excel(os.path.join(data_dir,'AVG_TAP.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colDict = {}\n",
    "for key in ['Sub005', 'Sub007']:\n",
    "    colDict[key] = 64\n",
    "for key in ['Sub021','Sub025','Sub028','Sub033','Sub050','Sub065']:\n",
    "    colDict[key] = 62\n",
    "for key in ['Sub029']:\n",
    "    colDict[key] = 72\n",
    "\n",
    "max_values_df1 = [0.219394344,\n",
    "0.028337412,\n",
    "0.170291695,\n",
    "0.022434686,\n",
    "0.025038189,\n",
    "0.097566164,\n",
    "0.038175539,\n",
    "0.0295194,\n",
    "0.198442069]\n",
    "\n",
    "max_values_df2 = [0.407060589155108,\n",
    "0.0348696339338623,\n",
    "0.230891052766452,\n",
    "0.0442756168720574,\n",
    "0.0333978157219401,\n",
    "0.121905995543275,\n",
    "0.0451621357921537,\n",
    "0.0475377818753191,\n",
    "0.174063343962767\n",
    "]\n",
    "\n",
    "statistic, p_value = mannwhitneyu(max_values_df1, max_values_df2)\n",
    "p_value_round = np.round(p_value, decimals = 6)\n",
    "\n",
    "print(p_value_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_rest_gamma = all_rest_df.iloc[50:81]\n",
    "#all_tap_gamma = all_tap_df.iloc[50:81]\n",
    "\n",
    "#rest_smoothed = all_rest_gamma.rolling(window=3, min_periods=1).mean()\n",
    "#tap_smoothed = all_tap_gamma.rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "#max_values_df1 = all_rest_gamma.max()\n",
    "df2_normalized = all_tap_df.div(max_values_df1)\n",
    "df2_normalized.columns = all_tap_df.columns\n",
    "df2_normalized.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to add NaN rows and shift values\n",
    "columns_63 = ['Sub021', 'Sub025', 'Sub028','Sub033', 'Sub050', 'Sub065']\n",
    "columns_73 = ['Sub029']\n",
    "\n",
    "df2_normalized2 = df2_normalized\n",
    "# Adding NaN rows and shifting values for specified columns\n",
    "for column in columns_63:\n",
    "    # Shift values down by the number of rows\n",
    "    df2_normalized2[column] = df2_normalized[column].shift(2)\n",
    "\n",
    "    # Add NaN rows to the beginning of the column\n",
    "    df2_normalized.loc[:2-1, column] = None\n",
    "\n",
    "for column1 in columns_73:\n",
    "    df2_normalized2[column1] = df2_normalized2[column1].shift(-7)\n",
    "    #df2_normalized2.loc[:10-1, column1] = None\n",
    "\n",
    "\n",
    "df2_normalized2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "mean_tap = np.nanmean(df2_normalized2, axis = 1)\n",
    "sem_tap = stats.sem(df2_normalized2, axis = 1, nan_policy= 'omit')\n",
    "\n",
    "plt.plot(np.arange(1,127), mean_tap, color = 'purple', lw = 2)\n",
    "plt.plot(np.arange(1,127), df2_normalized2, color = 'purple', alpha = 0.18)\n",
    "plt.fill_between(np.arange(1,127),mean_tap - sem_tap, mean_tap + sem_tap, color = 'purple', alpha=0.3, linestyle = 'dashed', lw = 1.5)\n",
    "plt.axhline(y = 1, color = 'grey', linestyle = ':', alpha = 0.5)\n",
    "plt.xlim(50,80)\n",
    "plt.ylim(0,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_columns_with_transparency(dataframe):\n",
    "    # Line color and transparency\n",
    "    transparencies = np.linspace(0.3,1,9)\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot columns with increasing transparency\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "            dataframe[column].plot(ax=ax, color = 'purple', alpha=transparencies[i], label=f'{column}', lw = 2.5)\n",
    "\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_ylabel('LFP Power [Normalized to Rest]')\n",
    "    # Add a legend\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Plot the DataFrame columns\n",
    "plot_columns_with_transparency(df2_normalized)\n",
    "plt.axhline(y = 1, color = 'grey', linestyle = ':', alpha = 0.3)\n",
    "plt.xlim(50,80)\n",
    "plt.xticks(np.arange(50,85,5), labels = ['-15','-10','-5','1:2\\nEntrainment', '+5','+10','+15'])\n",
    "plt.ylim(0,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', 'Movement_Modulation_All'),\n",
    "    dpi=200)\n",
    "\n",
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', 'Movement_Modulation_All.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary Patient Movement Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_npy = np.load(os.path.join(\n",
    "    project_path,\n",
    "    'data', 'FFTs', 'with_med_FTG',\n",
    "    'Sub029_FFT.npy'\n",
    "))\n",
    "\n",
    "fft_npy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel = np.load('S:\\\\AG\\\\AG-Bewegungsstoerungen-II\\\\LFP\\\\PROJECTS\\\\ENTRAINMENT\\\\Accelerometer\\\\raw_csv_all\\\\Synced_traces\\\\Sub029_cut_traces.npy')\n",
    "accel.shape\n",
    "import dat_preproc\n",
    "faccel = dat_preproc.low_highpass_filter(accel, 1, 124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = np.full((1,323),1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax2 = ax[0].twinx()\n",
    "time_points = np.arange(accel.shape[1]) / 250\n",
    "\n",
    "ax[0].pcolormesh(fft_npy[1,:,:], cmap = 'viridis', vmin = 0, vmax = 0.8) #vmin = -4, vmax = 7)\n",
    "ax[0].set_xlim(100,200)\n",
    "ax[0].set_ylim(60,80)\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_yticklabels([])\n",
    "ax2.plot(np.arange(0, fft_npy.shape[2]), stim[0], color = 'white', linewidth = 2.5, linestyle = 'dotted')\n",
    "ax2.set_yticks(np.arange(1,3,0.5))\n",
    "ax2.set_yticklabels([])\n",
    "ax[1].plot(time_points, faccel[2,:], color = 'grey', alpha = 0.8)\n",
    "ax[1].set_xlim(100,200)\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "#plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', 'Sub029_ExemplaryMovementEmpty'),\n",
    "    dpi=200)\n",
    "\n",
    "'''plt.savefig(os.path.join(\n",
    "    project_path,\n",
    "    'figures',\n",
    "    'movement_modul', 'Sub029_ExemplaryMovement.jpg'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import mne  # Import the mne module if not already imported\n",
    "\n",
    "def save_pdf_without_antialiasing(file_path, fig, dpi=100):\n",
    "    with PdfPages(file_path) as pdf:\n",
    "        # Save the figure as a PDF without anti-aliasing\n",
    "        pdf.savefig(fig, dpi=dpi)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    # Replace 'figure.pdf' with the desired file path and name for your PDF file\n",
    "    file_path = \"figure.pdf\"\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8.5, 11))\n",
    "    bs_data = mne.baseline.rescale(data=fft_npy[1, :, :], times=np.arange(0, fft_npy.shape[2]), baseline=baseline, mode='zscore')\n",
    "\n",
    "    ax[0].pcolormesh(fft_npy[1, :, :], cmap='viridis', vmin=0, vmax=0.8)\n",
    "    ax[0].set_xlim(100, 200)\n",
    "    ax[0].set_ylim(60, 80)\n",
    "    ax[0].set_ylabel('Frequency [Hz]')\n",
    "\n",
    "    ax[1].plot(time_points, faccel[2, :], color='grey', alpha=0.8)\n",
    "    ax[1].set_xlim(100, 200)\n",
    "    ax[1].set_ylabel('Acceleration [g]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "\n",
    "    save_pdf_without_antialiasing(file_path, fig)\n",
    "\n",
    "    # Show the plot (optional)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
